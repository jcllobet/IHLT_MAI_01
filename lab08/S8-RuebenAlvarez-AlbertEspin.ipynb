{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Human Language Technology - Lab. Session 8: Parsing\n",
    "### Authors: Rueben Álvarez and Albert Espín"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Constituency parsing with NLTK\n",
    "This section will define a grammar and use different parsing methods to determine the constituency tree of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Grammar definition\n",
    "The following cell defines a grammar that will allow to parse sentences with very specific terms and relations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import CFG, ChartParser\n",
    "\n",
    "'''the grammar is expanded considering that:\n",
    "    - sentences are composed by noun and verb prhases\n",
    "    - \"mice\" is added as a possible plural name (NNS)\n",
    "    - \"with\" is added as a possible preposition (CC)\n",
    "    - \"play\" is added as the only possible verb (V)\n",
    "    - the verb phrase (VP) is defined as a verb (V) or a verb plus a prepositional phrase (V PP)\n",
    "    - the prepositional phrase is defined as a preposition plus a noun phrase (CC NP)\n",
    "'''\n",
    "grammar = CFG.fromstring('''\n",
    "                        S -> NP VP\n",
    "                        NP -> NNS | JJ NNS | NP CC NP \n",
    "                        NNS -> \"cats\" | \"dogs\" | \"mice\" | NNS CC NNS\n",
    "                        JJ -> \"big\" | \"small\" | \"lazy\"\n",
    "                        CC -> \"and\" | \"or\" | \"with\"\n",
    "                        V -> \"play\"\n",
    "                        VP -> V PP | V\n",
    "                        PP -> CC NP\n",
    "                        ''')\n",
    "\n",
    "# sentence tokens\n",
    "sentence = nltk.word_tokenize(\"Lazy cats play with mice\".lower())\n",
    "\n",
    "# launch the server in terminal\n",
    "# java -mx4g -cp \"/home/jan/Downloads/stanford-corenlp-full-2018-10-05/*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Constituency parsing\n",
    "The following cells perform the constituency parsing using a BottomUpChartParser, a BottomUpLeftCornerChartParser and a LeftCornerChartParser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence to parse: ['lazy', 'cats', 'play', 'with', 'mice']\n",
      "\n",
      "Parsing with BottomUpChartParser:\n",
      "Number of trees:  1\n",
      "Parsed tree:  (S\n",
      "  (NP (JJ lazy) (NNS cats))\n",
      "  (VP (V play) (PP (CC with) (NP (NNS mice)))))\n",
      "There are 50 edges:\n",
      "[0:1] 'lazy'\n",
      "[1:2] 'cats'\n",
      "[2:3] 'play'\n",
      "[3:4] 'with'\n",
      "[4:5] 'mice'\n",
      "[0:0] JJ -> * 'lazy'\n",
      "[0:1] JJ -> 'lazy' *\n",
      "[0:0] NP -> * JJ NNS\n",
      "[0:1] NP -> JJ * NNS\n",
      "[1:1] NNS -> * 'cats'\n",
      "[1:2] NNS -> 'cats' *\n",
      "[1:1] NP -> * NNS\n",
      "[1:1] NNS -> * NNS CC NNS\n",
      "[0:2] NP -> JJ NNS *\n",
      "[1:2] NP -> NNS *\n",
      "[1:2] NNS -> NNS * CC NNS\n",
      "[1:1] S  -> * NP VP\n",
      "[1:1] NP -> * NP CC NP\n",
      "[1:2] S  -> NP * VP\n",
      "[1:2] NP -> NP * CC NP\n",
      "[0:0] S  -> * NP VP\n",
      "[0:0] NP -> * NP CC NP\n",
      "[0:2] S  -> NP * VP\n",
      "[0:2] NP -> NP * CC NP\n",
      "[2:2] V  -> * 'play'\n",
      "[2:3] V  -> 'play' *\n",
      "[2:2] VP -> * V PP\n",
      "[2:2] VP -> * V\n",
      "[2:3] VP -> V * PP\n",
      "[2:3] VP -> V *\n",
      "[1:3] S  -> NP VP *\n",
      "[0:3] S  -> NP VP *\n",
      "[3:3] CC -> * 'with'\n",
      "[3:4] CC -> 'with' *\n",
      "[3:3] PP -> * CC NP\n",
      "[3:4] PP -> CC * NP\n",
      "[4:4] NNS -> * 'mice'\n",
      "[4:5] NNS -> 'mice' *\n",
      "[4:4] NP -> * NNS\n",
      "[4:4] NNS -> * NNS CC NNS\n",
      "[4:5] NP -> NNS *\n",
      "[4:5] NNS -> NNS * CC NNS\n",
      "[4:4] S  -> * NP VP\n",
      "[4:4] NP -> * NP CC NP\n",
      "[3:5] PP -> CC NP *\n",
      "[4:5] S  -> NP * VP\n",
      "[4:5] NP -> NP * CC NP\n",
      "[2:5] VP -> V PP *\n",
      "[1:5] S  -> NP VP *\n",
      "[0:5] S  -> NP VP *\n",
      "\n",
      "\n",
      "Parsing with BottomUpLeftCornerChartParser:\n",
      "Number of trees:  1\n",
      "Parsed tree:  (S\n",
      "  (NP (JJ lazy) (NNS cats))\n",
      "  (VP (V play) (PP (CC with) (NP (NNS mice)))))\n",
      "There are 31 edges:\n",
      "[0:1] 'lazy'\n",
      "[1:2] 'cats'\n",
      "[2:3] 'play'\n",
      "[3:4] 'with'\n",
      "[4:5] 'mice'\n",
      "[0:1] JJ -> 'lazy' *\n",
      "[0:1] NP -> JJ * NNS\n",
      "[1:2] NNS -> 'cats' *\n",
      "[1:2] NP -> NNS *\n",
      "[1:2] NNS -> NNS * CC NNS\n",
      "[0:2] NP -> JJ NNS *\n",
      "[0:2] S  -> NP * VP\n",
      "[0:2] NP -> NP * CC NP\n",
      "[1:2] S  -> NP * VP\n",
      "[1:2] NP -> NP * CC NP\n",
      "[2:3] V  -> 'play' *\n",
      "[2:3] VP -> V * PP\n",
      "[2:3] VP -> V *\n",
      "[0:3] S  -> NP VP *\n",
      "[1:3] S  -> NP VP *\n",
      "[3:4] CC -> 'with' *\n",
      "[3:4] PP -> CC * NP\n",
      "[4:5] NNS -> 'mice' *\n",
      "[4:5] NP -> NNS *\n",
      "[4:5] NNS -> NNS * CC NNS\n",
      "[4:5] S  -> NP * VP\n",
      "[4:5] NP -> NP * CC NP\n",
      "[3:5] PP -> CC NP *\n",
      "[2:5] VP -> V PP *\n",
      "[0:5] S  -> NP VP *\n",
      "[1:5] S  -> NP VP *\n",
      "Edges that were filtered out:\n",
      "[0:0] JJ -> * 'lazy'\n",
      "[0:0] NP -> * JJ NNS\n",
      "[1:1] NNS -> * 'cats'\n",
      "[1:1] NP -> * NNS\n",
      "[1:1] NNS -> * NNS CC NNS\n",
      "[1:1] S  -> * NP VP\n",
      "[1:1] NP -> * NP CC NP\n",
      "[0:0] S  -> * NP VP\n",
      "[0:0] NP -> * NP CC NP\n",
      "[2:2] V  -> * 'play'\n",
      "[2:2] VP -> * V PP\n",
      "[2:2] VP -> * V\n",
      "[3:3] CC -> * 'with'\n",
      "[3:3] PP -> * CC NP\n",
      "[4:4] NNS -> * 'mice'\n",
      "[4:4] NP -> * NNS\n",
      "[4:4] NNS -> * NNS CC NNS\n",
      "[4:4] S  -> * NP VP\n",
      "[4:4] NP -> * NP CC NP\n",
      "\n",
      "\n",
      "Parsing with LeftCornerChartParser:\n",
      "Number of trees:  1\n",
      "Parsed tree:  (S\n",
      "  (NP (JJ lazy) (NNS cats))\n",
      "  (VP (V play) (PP (CC with) (NP (NNS mice)))))\n",
      "There are 25 edges:\n",
      "[0:1] 'lazy'\n",
      "[1:2] 'cats'\n",
      "[2:3] 'play'\n",
      "[3:4] 'with'\n",
      "[4:5] 'mice'\n",
      "[0:1] JJ -> 'lazy' *\n",
      "[0:1] NP -> JJ * NNS\n",
      "[1:2] NNS -> 'cats' *\n",
      "[1:2] NP -> NNS *\n",
      "[0:2] NP -> JJ NNS *\n",
      "[0:2] S  -> NP * VP\n",
      "[1:2] S  -> NP * VP\n",
      "[2:3] V  -> 'play' *\n",
      "[2:3] VP -> V * PP\n",
      "[2:3] VP -> V *\n",
      "[0:3] S  -> NP VP *\n",
      "[1:3] S  -> NP VP *\n",
      "[3:4] CC -> 'with' *\n",
      "[3:4] PP -> CC * NP\n",
      "[4:5] NNS -> 'mice' *\n",
      "[4:5] NP -> NNS *\n",
      "[3:5] PP -> CC NP *\n",
      "[2:5] VP -> V PP *\n",
      "[0:5] S  -> NP VP *\n",
      "[1:5] S  -> NP VP *\n",
      "Edges that were filtered out:\n",
      "[0:0] JJ -> * 'lazy'\n",
      "[0:0] NP -> * JJ NNS\n",
      "[1:1] NNS -> * 'cats'\n",
      "[1:1] NP -> * NNS\n",
      "[1:1] NNS -> * NNS CC NNS\n",
      "[1:2] NNS -> NNS * CC NNS\n",
      "[1:1] S  -> * NP VP\n",
      "[1:1] NP -> * NP CC NP\n",
      "[1:2] NP -> NP * CC NP\n",
      "[0:0] S  -> * NP VP\n",
      "[0:0] NP -> * NP CC NP\n",
      "[0:2] NP -> NP * CC NP\n",
      "[2:2] V  -> * 'play'\n",
      "[2:2] VP -> * V PP\n",
      "[2:2] VP -> * V\n",
      "[3:3] CC -> * 'with'\n",
      "[3:3] PP -> * CC NP\n",
      "[4:4] NNS -> * 'mice'\n",
      "[4:4] NP -> * NNS\n",
      "[4:4] NNS -> * NNS CC NNS\n",
      "[4:5] NNS -> NNS * CC NNS\n",
      "[4:4] S  -> * NP VP\n",
      "[4:4] NP -> * NP CC NP\n",
      "[4:5] S  -> NP * VP\n",
      "[4:5] NP -> NP * CC NP\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import BottomUpChartParser, ChartParser, LeftCornerChartParser\n",
    "\n",
    "all_edges = []\n",
    "\n",
    "print(\"Sentence to parse: {}\\n\".format(sentence))\n",
    "\n",
    "# use different parser classes\n",
    "for parser_class, parser_name in [(BottomUpChartParser, \"BottomUpChartParser\"), (ChartParser, \"BottomUpLeftCornerChartParser\"), (LeftCornerChartParser, \"LeftCornerChartParser\")]:\n",
    "\n",
    "    # use the grammar to create a parser, and parse the sentence with it  \n",
    "    parser = parser_class(grammar)\n",
    "    parse = parser.parse(sentence)\n",
    "\n",
    "    print(\"Parsing with {}:\".format(parser_name))\n",
    "\n",
    "    # show the possible constituency trees\n",
    "    trees = []\n",
    "    for tree in parse:\n",
    "        trees.append(tree)\n",
    "    print(\"Number of trees: \", len(trees))\n",
    "    for tree in trees:\n",
    "        print(\"Parsed tree: \", tree)\n",
    "        \n",
    "    # show the edges that were found and those that were filtered out\n",
    "    parse = parser.chart_parse(sentence)\n",
    "    print(\"There are {} edges:\".format(parse.num_edges()))\n",
    "    edges = parse.edges()\n",
    "    for edge in edges:\n",
    "        print(edge)\n",
    "    if parser_class == BottomUpChartParser:\n",
    "        all_edges = edges\n",
    "    else:\n",
    "        print(\"Edges that were filtered out:\")\n",
    "        for edge in all_edges:\n",
    "            if edge not in edges:\n",
    "                print(edge)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Discussion of the results\n",
    "\n",
    "All three used parsers (BottomUpChartParser, a BottomUpLeftCornerChartParser and a LeftCornerChartParser) produce the same constituency parsing tree for the analyzed sentence (\"Lazy cats play with mice\"). The result is a sentence composed of a noun phrase (\"lazy cats\") and a verb phrase (\"play with mice\"). The main noun prase is made up of an adjective (\"lazy\") and a a plural noun (\"cats\"). The verb phrase is composed of two elements: a verb (\"play\") and a propositional phrase (\"with mice\"), which is composed of a preposition (\"with\") and a noun phrase which is a plural noun (\"mice\").\n",
    "\n",
    "The different between the parsers is not found in the obtained tree, but on the way how it is obtained and how individual edges are filtered according with the principles of each parsing technique. BottomUpChartParser applies no filtering, so it constructs the tree from bottom level (individual words) to top level (the whole sentence), so it ends up producing 50 edges.\n",
    "\n",
    "Both BottomUpLeftCornerChartParser and LeftCornerChartParser are more efficient, since they filter some edges. The exact list of filtered edges can be seen in the previous section. BottomUpLeftCornerChartParser filters out edges without any word subsumption, such as \"* 'lazy'\", since there is no subsumption for \"lazy\" in the sentence, or \"* 'play'\", since there is no subsumption for \"play\". Thanks to this, BottomUpLeftCornerChartParser considers only 31 edges. LeftCornerChartParser is the most efficient of the three parsers, obtaining 25 edges in total. It achieves this by filtering out not only edges without any word subsumption (like the already mentioned \"* 'lazy'\") just like BottomUpLeftCornerChartParser did, but also edges without any new word subsumption. This is the case, for example, of \"NP * CC NP\", since the subsumptions shown there were already represented in previous edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dependency parsing with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sentence pairs and depencency parsing.\n",
    "The following cell finds the pairs of sentences in the file \"STS.input.txt\" of the trial corpus. Afterwards, it stores the pairs as tuples (i.e. each sentence of the pair becomes a tuple element), with each sentence transformed into a set of dependency triples generated with a depencency parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-afbb9f6af461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtriples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_dependency_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Smith jumps over the lazy dog\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-afbb9f6af461>\u001b[0m in \u001b[0;36mget_dependency_triples\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# generate the dependency tree using NLP Dependency Parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPDependencyParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://localhost:9000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/IHLT/lib/python3.7/site-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, url, encoding, tagtype)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'http://localhost:9000'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "\n",
    "def get_dependency_triples(sentence):\n",
    "    \n",
    "    \"\"\"Returns an array with the triples of depencency parsing for the passed sentence\"\"\"\n",
    "    \n",
    "    # generate the dependency tree using NLP Dependency Parser\n",
    "    parser = CoreNLPDependencyParser(\"http://localhost:9000\")\n",
    "    parse = parser.raw_parse(sentence)\n",
    "    \n",
    "    # extract the triples from the depencency tree\n",
    "    triples = []\n",
    "    tree = next(parse)\n",
    "    for triple in tree.triples():\n",
    "        triples.append(triple)\n",
    "    return triples\n",
    "    \n",
    "print(get_dependency_triples(\"Smith jumps over the lazy dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# full path of the corpus file, assuming that the trial folder containing the input file is in the same directory as the \"ipython\" file\n",
    "absolute_file_path = os.path.dirname(os.path.abspath(\"__file__\")) + \"//trial//STS.input.txt\"\n",
    "\n",
    "# find all sentence pairs in the document\n",
    "sentence_pairs = []\n",
    "sentence_set_pairs = []\n",
    "with open(absolute_file_path) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        index, sentence0, sentence1 = line.split(\"\\t\")\n",
    "        sentence_pairs.append((get_dependency_triples(sentence0), get_dependency_triples(sentence1)))\n",
    "        print(\"First sentence: \\t\", sentence0, \"\\nSecond sentence: \\t\", sentence1, \"\\n\")\n",
    "    print()  \n",
    "    \n",
    "# the pairs of sentences are shown\n",
    "for pair in sentence_pairs:\n",
    "    print(\"{}\\n{}\\n\\n\".format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentence similarity calculation using triples from dependency parsing\n",
    "The pairs of sentences are checked to see how similar they are, using the Jaccard distance: the more dependency triples two sentences share, the more alike they are considered to be. The pairs are shown along with their distance and dissimilarity score (scaled to be comparable with the gold standard one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import jaccard_distance\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# gold standard file path\n",
    "absolute_file_path = os.path.dirname(os.path.abspath(\"__file__\")) + \"//trial//STS.gs.txt\"\n",
    "\n",
    "# get the gold standard scores\n",
    "gold_scores = []\n",
    "with open(absolute_file_path) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        _, score = line.split(\"\\t\")\n",
    "        gold_scores.append(int(score))\n",
    "        \n",
    "dependency_scores = []\n",
    "\n",
    "# compute the Jaccard distance to see how similar or different two sentences are\n",
    "for i in range(len(sentence_pairs)):\n",
    "    pair = sentence_pairs[i]\n",
    "    dependency_dist = jaccard_distance(set(pair[0]), set(pair[1]))\n",
    "    dependency_score = round(dependency_dist * 5)\n",
    "    dependency_scores.append(dependency_score)\n",
    "    print(\"First sentence dependency triples: \", pair[0], \"\\nSecond sentence dependency triples: \", pair[1], \"\\nDepencency-triples-based distance:\", round(dependency_dist, 3), \"\\nDepencency-triples-based dissimilarity score:\", dependency_score, \"\\nGold standard dissimilarity score:\", gold_scores[i], \"\\n\") \n",
    "\n",
    "# Pearson correlation between the tested and the gold standard scores\n",
    "word_ne_pearson = pearsonr(dependency_scores, gold_scores)\n",
    "print(\"Pearson correlation between dependency-triples-based method and gold standard:\", round(word_ne_pearson[0], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explanation of the results of sentence similarity measure using dependency triples, compared to the gold standard.\n",
    "The Pearson correlation between the gold standard and the method that calculates the similarity of two sentences checking the proportion of common dependency triples is close to zero (-0.131), showing that there is very little correlation. In other words, dependency triples alone are not informative enough to determine how similar two sentences are. For two triples to be considered equal and contribute to considering the sentence pairs similar, all the three elements in the triple (the two words and their syntactic relation) must be equal. This does not happen if at least one of the elements, e.g. one of the two words, is different, even if it represents the same meaning. To compute how similar two sentences are with Jaccard distance, it is better to use other approaches, such as word, lemma or synset-based comparisons, as shown in previous sessions, that allowed to obtain Pearson correlations of a higher absolute value.\n",
    "\n",
    "Dependency parsing, however, would be usable in syntactic similarity comparison between sentences, by analyzing the relations that appear in the dependency tree, independently of the particular words involved in the sentences. The proportion of common relations can be analyzed easily by using the Jaccard distance on the full set of dependency tree relations (i.e. the relation element of each one of the dependency triples, instead of the whole triple). This naive calculation, however, would not take into account the ordering (from general to specific in the sentence, for example) of the relations. A tree exploration algorithm, such a modified version of breadth-first search, may be used to check how many relations appear in the same level of both trees, or after the same parent relations. This would give a much more detailed view on the syntactic similarity of the sentences, but not taking into account their semantic similarity.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
