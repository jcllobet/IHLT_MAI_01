{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Exercise - Session 5\n",
    "\n",
    "### Students: Nafis Banirazi & Jan Carbonell\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this lab is to categorize the given pairs, print their most frequent WordNet synset, their corresponding least common subsumer (LCS) and their similarity using the following functions:\n",
    "\n",
    "- Path Similarity\n",
    "- Leacock-Chodorow Similarity\n",
    "- Wu-Palmer Similarity\n",
    "- Lin Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports. Could also be done in the PC\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "#additional set of imports\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "\n",
    "#given set of pairs\n",
    "pairs = [('the', 'DT'), ('man', 'NN'), ('swim', 'VB'), \\\n",
    "         ('with', 'PR'), ('a', 'DT'), ('girl', 'NN'), \\\n",
    "         ('and', 'CC'), ('a', 'DT'), ('boy', 'NN'), \\\n",
    "         ('whilst', 'PR'), ('the', 'DT'), ('woman', 'NN'), \\\n",
    "         ('walk', 'VB')]\n",
    "n = {}\n",
    "v = {}\n",
    "aux = {}\n",
    "freq = []\n",
    "lcs = []\n",
    "definition = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each pair, we will search for their most frequent WordNet sysnset. In the documentation we can find that there are also adjectives and adverbs listed as options but are not used in this given set of pairs. Listing it as a reference: https://wordnet.princeton.edu/documentation/wn1wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man Synset('man.n.01')\n",
      "girl Synset('girl.n.01')\n",
      "boy Synset('male_child.n.01')\n",
      "woman Synset('woman.n.01')\n",
      "swim Synset('swim.v.01')\n",
      "walk Synset('walk.v.01')\n"
     ]
    }
   ],
   "source": [
    "for e in pairs:\n",
    "    if e[0] not in n and e[0] not in v:\n",
    "        if e[1] == 'NN':\n",
    "            n[e[0]] = wn.synset(e[0]+'.n.01')\n",
    "        elif e[1] == 'VB':\n",
    "            v[e[0]] = wn.synset(e[0]+'.v.01')\n",
    "            \n",
    "#verification that it is properly stored\n",
    "for keys,values in n.items():\n",
    "    print(keys, values)\n",
    "    \n",
    "for keys,values in v.items():\n",
    "    print(keys, values)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the pairs that was found to be on WordNet, we now get their corresponding least commmon subsumer. That is the most specific common ancestor (hypernym) of two concepts found in a given ontology. For example, the LCS of moose and kangaroo in WordNet is mammal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in n:\n",
    "    freq.append([key,n[key], 'noun'])\n",
    "\n",
    "for key in v:\n",
    "    freq.append([key,v[key], 'verb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[Synset('man.n.01')], [Synset('adult.n.01')], [Synset('male.n.02')], [Synset('adult.n.01')], '-', '-'], [[Synset('adult.n.01')], [Synset('girl.n.01')], [Synset('person.n.01')], [Synset('woman.n.01')], '-', '-'], [[Synset('male.n.02')], [Synset('person.n.01')], [Synset('male_child.n.01')], [Synset('person.n.01')], '-', '-'], [[Synset('adult.n.01')], [Synset('woman.n.01')], [Synset('person.n.01')], [Synset('woman.n.01')], '-', '-'], ['-', '-', '-', '-', [Synset('swim.v.01')], [Synset('travel.v.01')]], ['-', '-', '-', '-', [Synset('travel.v.01')], [Synset('walk.v.01')]]]\n"
     ]
    }
   ],
   "source": [
    "for key in freq:\n",
    "    row = []\n",
    "    for alt in freq:\n",
    "        if key[2] == alt [2]:\n",
    "            row.append(key[1].lowest_common_hypernyms(alt[1]))\n",
    "        else:\n",
    "            row.append('-')\n",
    "    lcs.append(row)\n",
    "\n",
    "print(lcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>man</th>\n",
       "      <th>girl</th>\n",
       "      <th>boy</th>\n",
       "      <th>woman</th>\n",
       "      <th>swim</th>\n",
       "      <th>walk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>[Synset('man.n.01')]</td>\n",
       "      <td>[Synset('adult.n.01')]</td>\n",
       "      <td>[Synset('male.n.02')]</td>\n",
       "      <td>[Synset('adult.n.01')]</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girl</th>\n",
       "      <td>[Synset('adult.n.01')]</td>\n",
       "      <td>[Synset('girl.n.01')]</td>\n",
       "      <td>[Synset('person.n.01')]</td>\n",
       "      <td>[Synset('woman.n.01')]</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boy</th>\n",
       "      <td>[Synset('male.n.02')]</td>\n",
       "      <td>[Synset('person.n.01')]</td>\n",
       "      <td>[Synset('male_child.n.01')]</td>\n",
       "      <td>[Synset('person.n.01')]</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>[Synset('adult.n.01')]</td>\n",
       "      <td>[Synset('woman.n.01')]</td>\n",
       "      <td>[Synset('person.n.01')]</td>\n",
       "      <td>[Synset('woman.n.01')]</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swim</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>[Synset('swim.v.01')]</td>\n",
       "      <td>[Synset('travel.v.01')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>[Synset('travel.v.01')]</td>\n",
       "      <td>[Synset('walk.v.01')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          man                     girl  \\\n",
       "man      [Synset('man.n.01')]   [Synset('adult.n.01')]   \n",
       "girl   [Synset('adult.n.01')]    [Synset('girl.n.01')]   \n",
       "boy     [Synset('male.n.02')]  [Synset('person.n.01')]   \n",
       "woman  [Synset('adult.n.01')]   [Synset('woman.n.01')]   \n",
       "swim                        -                        -   \n",
       "walk                        -                        -   \n",
       "\n",
       "                               boy                    woman  \\\n",
       "man          [Synset('male.n.02')]   [Synset('adult.n.01')]   \n",
       "girl       [Synset('person.n.01')]   [Synset('woman.n.01')]   \n",
       "boy    [Synset('male_child.n.01')]  [Synset('person.n.01')]   \n",
       "woman      [Synset('person.n.01')]   [Synset('woman.n.01')]   \n",
       "swim                             -                        -   \n",
       "walk                             -                        -   \n",
       "\n",
       "                          swim                     walk  \n",
       "man                          -                        -  \n",
       "girl                         -                        -  \n",
       "boy                          -                        -  \n",
       "woman                        -                        -  \n",
       "swim     [Synset('swim.v.01')]  [Synset('travel.v.01')]  \n",
       "walk   [Synset('travel.v.01')]    [Synset('walk.v.01')]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs_np = np.array(lcs)\n",
    "label = [i[0] for i in freq]\n",
    "pd.DataFrame(lcs_np, columns = label, index= label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to implement and evaluate the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('entity.n.01')] 0.6418538861723948\n"
     ]
    }
   ],
   "source": [
    "dog = wn.synset('plantation.n.01')\n",
    "swim = wn.synset('dinosaur.n.01')\n",
    "a = dog.lowest_common_hypernyms(swim)\n",
    "b = wn.lch_similarity(dog, swim)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.25, 0.333, 0.333, '-', '-'], [0.25, 1.0, 0.167, 0.5, '-', '-'], [0.333, 0.167, 1.0, 0.2, '-', '-'], [0.333, 0.5, 0.2, 1.0, '-', '-'], ['-', '-', '-', '-', 1.0, 0.333], ['-', '-', '-', '-', 0.333, 1.0]]\n",
      "[[1.0, 0.619, 0.698, 0.698, '-', '-'], [0.619, 1.0, 0.507, 0.809, '-', '-'], [0.698, 0.507, 1.0, 0.558, '-', '-'], [0.698, 0.809, 0.558, 1.0, '-', '-'], ['-', '-', '-', '-', 1.0, 0.663], ['-', '-', '-', '-', 0.663, 1.0]]\n",
      "[[1.0, 1.0, 0.632, 0.702, 0.667, 0.798, 0.667, 0.802, '-', '-'], [0.632, 0.702, 1.0, 1.0, 0.632, 0.274, 0.632, 0.882, '-', '-'], [0.667, 0.798, 0.632, 0.274, 1.0, 1.0, 0.667, 0.308, '-', '-'], [0.667, 0.802, 0.947, 0.882, 0.667, 0.308, 1.0, 1.0, '-', '-'], ['-', '-', '-', '-', 1.0, 1.0, 0.333, 0.466], ['-', '-', '-', '-', 0.333, 0.466, 1.0, 1.0]]\n",
      "[[1.0, 1.0, 0.632, 0.702, 0.667, 0.798, 0.667, 0.802, '-', '-'], [0.632, 0.702, 1.0, 1.0, 0.632, 0.274, 0.632, 0.882, '-', '-'], [0.667, 0.798, 0.632, 0.274, 1.0, 1.0, 0.667, 0.308, '-', '-'], [0.667, 0.802, 0.947, 0.882, 0.667, 0.308, 1.0, 1.0, '-', '-'], ['-', '-', '-', '-', 1.0, 1.0, 0.333, 0.466], ['-', '-', '-', '-', 0.333, 0.466, 1.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "path, lch, wup, lin = [], [], [], []\n",
    "v1, v2, v3, v4 = 0, 0, 0, 0\n",
    "\n",
    "# initializing two words only connected by entity\n",
    "a = wn.synset('plantation.n.01')\n",
    "b = wn.synset('dinosaur.n.01')\n",
    "relation = a.lowest_common_hypernyms(b)\n",
    "\n",
    "#calculating max values as the result of running the algorithms on the same element\n",
    "max1 = wn.path_similarity(a, a)\n",
    "max2 = wn.lch_similarity(a, a)\n",
    "max3 = wn.wup_similarity(a, a)\n",
    "max4 = a.lin_similarity(a, semcor_ic)\n",
    "\n",
    "#calculating min value of the algorithms with words only connected by entity\n",
    "min1 = wn.lch_similarity(a, b)\n",
    "min2 = wn.lch_similarity(a, b)\n",
    "min3 = wn.wup_similarity(a, b)\n",
    "min4 = a.lin_similarity(b, semcor_ic)\n",
    "\n",
    "#max and min done outside the loop to preserve as we loop through elements\n",
    "\n",
    "for key in freq:\n",
    "    \n",
    "    #initializing the rows of the matrices\n",
    "    row1 = []\n",
    "    row2 = []\n",
    "    row3 = []\n",
    "    row4 = []\n",
    "    \n",
    "    for alt in freq:\n",
    "        #adding only if they belong to the same class 'noun' // 'verb'\n",
    "        if key[2] == alt[2]:\n",
    "            #calculating the result of the algorithm\n",
    "            v1 = wn.path_similarity(key[1], alt[1])\n",
    "            v2 = wn.lch_similarity(key[1], alt[1])\n",
    "            v3 = wn.wup_similarity(key[1], alt[1])\n",
    "            v4 = key[1].lin_similarity(alt[1], semcor_ic)\n",
    "            \n",
    "            #normalizing it, rounding 3 decimals and appending to row\n",
    "            row1.append(round(v1/max1, 3))\n",
    "            row2.append(round(v2/max2, 3))\n",
    "            row3.append(round(v3/max3, 3))\n",
    "            row3.append(round(v4/max4, 3))\n",
    "            \n",
    "        else:\n",
    "            row1.append('-')\n",
    "            row2.append('-')\n",
    "            row3.append('-')\n",
    "            row4.append('-')\n",
    "    path.append(row1)\n",
    "    lch.append(row2)\n",
    "    wup.append(row3)\n",
    "    lin.append(row4)\n",
    "\n",
    "print(path)\n",
    "print(lch)\n",
    "print(wup)\n",
    "print(wup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Which model would you select?</b> Justify the answer.\n",
    "\n",
    "Based on the initial graph, both CRF and PER seem like the better performing algorithms. If we could only make a decision based on this data, we would pick the Perceptron model.\n",
    "\n",
    "In order to pick the best overall model, another relevant measure is the speed of the system. Because of this, we also have implemented a timer in between each of the models and will plot it accordingly to figure out which algorithms were more effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing and plotting the accuracy graph\n",
    "x = train_stop\n",
    "plt.figure()\n",
    "plt.plot(x, time['HMM'], label='HMM')\n",
    "plt.plot(x, time['TnT'], label='TnT')\n",
    "plt.plot(x, time['PER'], label='PER')\n",
    "plt.plot(x, time['CRF'], label='CRF')\n",
    "\n",
    "#adding the legend showing the plot\n",
    "plt.xlabel('Number of Sentences')\n",
    "plt.ylabel('Model Execution Time')\n",
    "plt.title('Part Of Speech Models')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, PER seems to be the most effective -accuracy wise- and second most time efficient algorithm. After having performed this additional execution time analysis <b> we are reinsured in our conclusion to pick the Perceptron Model.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Over the development of this lab, we have implemented 4 different POS Models, tested them with different segments of the treebank corpus and trained them with another set of segments. From those results, we have plotted their performanced based on accuracy and execution time and have selected the best performing model for this specific case, which happened to be the Perceptron Model. \n",
    "\n",
    "Another thing to highlight is that the HiddenMarkovModel looks really efficient timewise. It is likely that its just not in the desired set of conditions regarding the amount of data to perform correctly. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
