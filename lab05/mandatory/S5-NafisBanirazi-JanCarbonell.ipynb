{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Exercise - Session 5\n",
    "\n",
    "### Students: Nafis Banirazi & Jan Carbonell\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this lab is to categorize the given pairs, print their most frequent WordNet synset, their corresponding least common subsumer (LCS) and their similarity using the following functions:\n",
    "\n",
    "- Path Similarity\n",
    "- Leacock-Chodorow Similarity\n",
    "- Wu-Palmer Similarity\n",
    "- Lin Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports. Could also be done in the PC\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "#additional set of imports\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "\n",
    "#given set of pairs\n",
    "pairs = [('the', 'DT'), ('man', 'NN'), ('swim', 'VB'), \\\n",
    "         ('with', 'PR'), ('a', 'DT'), ('girl', 'NN'), \\\n",
    "         ('and', 'CC'), ('a', 'DT'), ('boy', 'NN'), \\\n",
    "         ('whilst', 'PR'), ('the', 'DT'), ('woman', 'NN'), \\\n",
    "         ('walk', 'VB')]\n",
    "n = {}\n",
    "v = {}\n",
    "aux = {}\n",
    "freq = []\n",
    "lcs = []\n",
    "definition = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each pair, we will search for their most frequent WordNet sysnset. In the documentation we can find that there are also adjectives and adverbs listed as options but are not used in this given set of pairs. Listing it as a reference: https://wordnet.princeton.edu/documentation/wn1wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man Synset('man.n.01')\n",
      "girl Synset('girl.n.01')\n",
      "boy Synset('male_child.n.01')\n",
      "woman Synset('woman.n.01')\n",
      "swim Synset('swim.v.01')\n",
      "walk Synset('walk.v.01')\n"
     ]
    }
   ],
   "source": [
    "for e in pairs:\n",
    "    if e[0] not in n and e[0] not in v:\n",
    "        if e[1] == 'NN':\n",
    "            n[e[0]] = wn.synset(e[0]+'.n.01')\n",
    "        elif e[1] == 'VB':\n",
    "            v[e[0]] = wn.synset(e[0]+'.v.01')\n",
    "            \n",
    "#verification that it is properly stored\n",
    "for keys,values in n.items():\n",
    "    print(keys, values)\n",
    "    \n",
    "for keys,values in v.items():\n",
    "    print(keys, values)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the pairs that was found to be on WordNet, we now get their corresponding least commmon subsumer. That is the most specific common ancestor (hypernym) of two concepts found in a given ontology. For example, the LCS of moose and kangaroo in WordNet is mammal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in n:\n",
    "    freq.append([key,n[key], 'noun'])\n",
    "\n",
    "for key in v:\n",
    "    freq.append([key,v[key], 'verb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in freq:\n",
    "    row = []\n",
    "    for alt in freq:\n",
    "        if key[2] == alt [2]:\n",
    "            row.append(key[1].lowest_common_hypernyms(alt[1]))\n",
    "        else:\n",
    "            row.append(0)\n",
    "    lcs.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>man</th>\n",
       "      <th>girl</th>\n",
       "      <th>boy</th>\n",
       "      <th>woman</th>\n",
       "      <th>swim</th>\n",
       "      <th>walk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>[Synset('man.n.01')]</td>\n",
       "      <td>[Synset('adult.n.01')]</td>\n",
       "      <td>[Synset('male.n.02')]</td>\n",
       "      <td>[Synset('adult.n.01')]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girl</th>\n",
       "      <td>[Synset('adult.n.01')]</td>\n",
       "      <td>[Synset('girl.n.01')]</td>\n",
       "      <td>[Synset('person.n.01')]</td>\n",
       "      <td>[Synset('woman.n.01')]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boy</th>\n",
       "      <td>[Synset('male.n.02')]</td>\n",
       "      <td>[Synset('person.n.01')]</td>\n",
       "      <td>[Synset('male_child.n.01')]</td>\n",
       "      <td>[Synset('person.n.01')]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>[Synset('adult.n.01')]</td>\n",
       "      <td>[Synset('woman.n.01')]</td>\n",
       "      <td>[Synset('person.n.01')]</td>\n",
       "      <td>[Synset('woman.n.01')]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Synset('swim.v.01')]</td>\n",
       "      <td>[Synset('travel.v.01')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Synset('travel.v.01')]</td>\n",
       "      <td>[Synset('walk.v.01')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          man                     girl  \\\n",
       "man      [Synset('man.n.01')]   [Synset('adult.n.01')]   \n",
       "girl   [Synset('adult.n.01')]    [Synset('girl.n.01')]   \n",
       "boy     [Synset('male.n.02')]  [Synset('person.n.01')]   \n",
       "woman  [Synset('adult.n.01')]   [Synset('woman.n.01')]   \n",
       "swim                        0                        0   \n",
       "walk                        0                        0   \n",
       "\n",
       "                               boy                    woman  \\\n",
       "man          [Synset('male.n.02')]   [Synset('adult.n.01')]   \n",
       "girl       [Synset('person.n.01')]   [Synset('woman.n.01')]   \n",
       "boy    [Synset('male_child.n.01')]  [Synset('person.n.01')]   \n",
       "woman      [Synset('person.n.01')]   [Synset('woman.n.01')]   \n",
       "swim                             0                        0   \n",
       "walk                             0                        0   \n",
       "\n",
       "                          swim                     walk  \n",
       "man                          0                        0  \n",
       "girl                         0                        0  \n",
       "boy                          0                        0  \n",
       "woman                        0                        0  \n",
       "swim     [Synset('swim.v.01')]  [Synset('travel.v.01')]  \n",
       "walk   [Synset('travel.v.01')]    [Synset('walk.v.01')]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs_np = np.array(lcs)\n",
    "label = [i[0] for i in freq]\n",
    "pd.DataFrame(lcs_np, columns = label, index= label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#calculating min value of the algorithms with words only connected by entity\n",
    "min1 = wn.lch_similarity(a, b)\n",
    "min2 = wn.lch_similarity(a, b)\n",
    "min3 = wn.wup_similarity(a, b)\n",
    "min4 = a.lin_similarity(b, semcor_ic)\n",
    "\n",
    "# initializing two words only connected by entity\n",
    "a = wn.synset('plantation.n.01')\n",
    "b = wn.synset('dinosaur.n.01')\n",
    "relation = a.lowest_common_hypernyms(b)\n",
    "\n",
    "#max and min done outside the loop to preserve as we loop through elements\n",
    "\n",
    "dog = wn.synset('plantation.n.01')\n",
    "swim = wn.synset('dinosaur.n.01')\n",
    "a = dog.lowest_common_hypernyms(swim)\n",
    "b = wn.lch_similarity(dog, swim)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d38b62adcc8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# calculating max values as the result of running the algorithms on the same element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmax1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmax2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlch_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mmax3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwup_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmax4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemcor_ic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/IHLT_01/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mlch_similarity\u001b[0;34m(self, synset1, synset2, verbose, simulate_root)\u001b[0m\n\u001b[1;32m   1737\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulate_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     ):\n\u001b[0;32m-> 1739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msynset1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlch_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulate_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m     \u001b[0mlch_similarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlch_similarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/IHLT_01/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mlch_similarity\u001b[0;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wordnet_corpus_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             self._wordnet_corpus_reader._compute_max_depth(\n\u001b[0;32m--> 845\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m             )\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/IHLT_01/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_compute_max_depth\u001b[0;34m(self, pos, simulate_root)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \"\"\"\n\u001b[1;32m   1219\u001b[0m         \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m                 \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/IHLT_01/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mall_synsets\u001b[0;34m(self, pos)\u001b[0m\n\u001b[1;32m   1636\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m                             \u001b[0;31m# Otherwise, parse the line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1638\u001b[0;31m                             \u001b[0msynset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_pos_and_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1639\u001b[0m                             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/IHLT_01/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_synset_from_pos_and_line\u001b[0;34m(self, pos, data_file_line)\u001b[0m\n\u001b[1;32m   1469\u001b[0m         \u001b[0;31m# the canonical name is based on the first lemma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m         \u001b[0mlemma_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemmas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m         \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemma_pos_offset_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m         \u001b[0msense_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemma_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msense_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "path, lch, wup, lin = [], [], [], []\n",
    "v1, v2, v3, v4 = 0, 0, 0, 0\n",
    "max1, max2, max3, max4 = 0, 0, 0, 0\n",
    "min1, min2, min3, min4 = 0, 0, 0, 0\n",
    "\n",
    "\n",
    "for key in freq:\n",
    "    \n",
    "    #initializing the rows of the matrices\n",
    "    row1 = []\n",
    "    row2 = []\n",
    "    row3 = []\n",
    "    row4 = []\n",
    "    \n",
    "    # calculating max values as the result of running the algorithms on the same element\n",
    "    max1 = wn.path_similarity(key[1], key[1])\n",
    "    max2 = wn.lch_similarity(key[1], key[1])\n",
    "    max3 = wn.wup_similarity(key[1], key[1])\n",
    "    max4 = key[1].lin_similarity(key[1], semcor_ic)\n",
    "    \n",
    "    for alt in freq:\n",
    "        # adding only if they belong to the same class 'noun' // 'verb'\n",
    "        if key[2] == alt[2]:\n",
    "            \n",
    "            # calculating the result of the algorithm\n",
    "            v1 = wn.path_similarity(key[1], alt[1])\n",
    "            v2 = wn.lch_similarity(key[1], alt[1])\n",
    "            v3 = wn.wup_similarity(key[1], alt[1])\n",
    "            v4 = key[1].lin_similarity(alt[1], semcor_ic)\n",
    "            \n",
    "            # tracking the minimum value in the matrix\n",
    "            if min1 > v1:\n",
    "                min1 = v1\n",
    "            elif min2 > v2:\n",
    "                min2 = v2\n",
    "            elif min3 > v3:\n",
    "                min3 = v3\n",
    "            elif min4 > v4:\n",
    "                min4 = v4\n",
    "            \n",
    "            # rounding it and appending it to row\n",
    "            row1.append(round(v1, 3))\n",
    "            row2.append(round(v2, 3))\n",
    "            row3.append(round(v3, 3))\n",
    "            row4.append(round(v4, 3))\n",
    "            \n",
    "        else:\n",
    "            row1.append(0)\n",
    "            row2.append(0)\n",
    "            row3.append(0)\n",
    "            row4.append(0)\n",
    "    path.append(row1)\n",
    "    lch.append(row2)\n",
    "    wup.append(row3)\n",
    "    lin.append(row4)\n",
    "    \n",
    "print(path)\n",
    "print('\\n')\n",
    "print(lch)\n",
    "print('\\n')\n",
    "print(wup)\n",
    "print('\\n')\n",
    "print(lin)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in freq:\n",
    "    \n",
    "    #initializing the rows of the matrices\n",
    "    row1 = []\n",
    "    row2 = []\n",
    "    row3 = []\n",
    "    row4 = []\n",
    "    \n",
    "    for alt in freq:           \n",
    "        if key[2] == alt[2]:\n",
    "            # normalizing, rounding it to three decimals and appending it to row\n",
    "            (round((v1-min1)/max1, 3))\n",
    "            row2.append(round((v2-min2)/max2, 3))\n",
    "            row3.append(round((v3-min3)/max3, 3))\n",
    "            row4.append(round((v4-min4)/max4, 3))\n",
    "    path.append(row1)\n",
    "    lch.append(row2)\n",
    "    wup.append(row3)\n",
    "    lin.append(row4)\n",
    "    \n",
    "print(path)\n",
    "print('\\n')\n",
    "print(lch)\n",
    "print('\\n')\n",
    "print(wup)\n",
    "print('\\n')\n",
    "print(lin)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Which model would you select?</b> Justify the answer.\n",
    "\n",
    "Based on the initial graph, both CRF and PER seem like the better performing algorithms. If we could only make a decision based on this data, we would pick the Perceptron model.\n",
    "\n",
    "In order to pick the best overall model, another relevant measure is the speed of the system. Because of this, we also have implemented a timer in between each of the models and will plot it accordingly to figure out which algorithms were more effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_np = np.array(path)\n",
    "pd.DataFrame(path_np, columns = label, index= label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lch_np = np.array(lch)\n",
    "pd.DataFrame(lch_np, columns = label, index= label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wup_np = np.array(wup)\n",
    "pd.DataFrame(wup_np, columns = label, index= label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_np = np.array(lin)\n",
    "pd.DataFrame(lin_np, columns = label, index= label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, PER seems to be the most effective -accuracy wise- and second most time efficient algorithm. After having performed this additional execution time analysis <b> we are reinsured in our conclusion to pick the Perceptron Model.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Over the development of this lab, we have implemented 4 different POS Models, tested them with different segments of the treebank corpus and trained them with another set of segments. From those results, we have plotted their performanced based on accuracy and execution time and have selected the best performing model for this specific case, which happened to be the Perceptron Model. \n",
    "\n",
    "Another thing to highlight is that the HiddenMarkovModel looks really efficient timewise. It is likely that its just not in the desired set of conditions regarding the amount of data to perform correctly. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
