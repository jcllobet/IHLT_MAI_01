{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we import the required modules and open the input file in reading mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.wsd import lesk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "txt = open('../trial/STS.input.txt', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then, we read all the pairs of sentences of the trial set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id1': ['the bird is bathing in the sink.', 'birdie is washing itself in the water basin.'], 'id2': ['in may 2010, the troops attempted to invade kabul.', 'the us army invaded kabul on may 7th last year, 2010.'], 'id3': ['john said he is considered a witness but not a suspect.', '\"he is not a suspect anymore.\" john said.'], 'id4': ['they flew out of the nest in groups.', 'they flew into the nest together.'], 'id5': ['the woman is playing the violin.', 'the young lady enjoys listening to the guitar.'], 'id6': ['john went horse back riding at dawn with a whole group of friends.', 'sunrise at dawn is a magnificent view to take in if you wake up early enough for it.']}\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "for line in txt:\n",
    "    # lowercase the sentences\n",
    "    line = line.lower()\n",
    "    fields = line.strip().split('\\t')\n",
    "    d[fields[0]] = fields[1:]\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we tokenize the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id1': [['the', 'bird', 'is', 'bathing', 'in', 'the', 'sink', '.'], ['birdie', 'is', 'washing', 'itself', 'in', 'the', 'water', 'basin', '.']], 'id2': [['in', 'may', '2010', ',', 'the', 'troops', 'attempted', 'to', 'invade', 'kabul', '.'], ['the', 'us', 'army', 'invaded', 'kabul', 'on', 'may', '7th', 'last', 'year', ',', '2010', '.']], 'id3': [['john', 'said', 'he', 'is', 'considered', 'a', 'witness', 'but', 'not', 'a', 'suspect', '.'], ['``', 'he', 'is', 'not', 'a', 'suspect', 'anymore', '.', \"''\", 'john', 'said', '.']], 'id4': [['they', 'flew', 'out', 'of', 'the', 'nest', 'in', 'groups', '.'], ['they', 'flew', 'into', 'the', 'nest', 'together', '.']], 'id5': [['the', 'woman', 'is', 'playing', 'the', 'violin', '.'], ['the', 'young', 'lady', 'enjoys', 'listening', 'to', 'the', 'guitar', '.']], 'id6': [['john', 'went', 'horse', 'back', 'riding', 'at', 'dawn', 'with', 'a', 'whole', 'group', 'of', 'friends', '.'], ['sunrise', 'at', 'dawn', 'is', 'a', 'magnificent', 'view', 'to', 'take', 'in', 'if', 'you', 'wake', 'up', 'early', 'enough', 'for', 'it', '.']]}\n"
     ]
    }
   ],
   "source": [
    "for key in d:\n",
    "    d[key] = [nltk.word_tokenize(s) for s in d[key]]\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_pos(tag):\n",
    "    #if word is a noun\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    #if word is a verb\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    #if word is an adjective\n",
    "    elif tag.startswith('J'):\n",
    "        return 'a'\n",
    "    #if word is a adverb\n",
    "    elif tag.startswith('R'):\n",
    "        return 'b'\n",
    "\n",
    "def compute_lesk(d):\n",
    "    new_dict = {}\n",
    "    for key in d:\n",
    "        new_dict[key] = []\n",
    "        for sentence in d[key]:\n",
    "            new_sentence = []\n",
    "            for word in sentence:\n",
    "                pos = pos_tag([word])[0][1]\n",
    "                if pos[0] in {'N', 'V', 'J', 'R'}:\n",
    "                    synset = lesk(sentence, word, get_valid_pos(pos))\n",
    "                    if(synset is not None):\n",
    "                        split = synset.name().split('.')\n",
    "                        new_sentence.append(word + '/' + pos + '/' + split[-1])\n",
    "                    else:\n",
    "                        new_sentence.append(word + '/' + pos)\n",
    "                else:\n",
    "                    new_sentence.append(word + '/' + pos)\n",
    "            new_dict[key].append(new_sentence)\n",
    "    return(new_dict)\n",
    "\n",
    "def jaccard(d):\n",
    "    similarities = {}\n",
    "    print('Computed Jaccard similarity for each pair of sentences:')\n",
    "    for key in d:\n",
    "        sentences = d[key]\n",
    "        distance = jaccard_distance(set(sentences[0]), set(sentences[1]))\n",
    "        similarity = 1-distance\n",
    "        similarities[key] = [similarity]\n",
    "        print(key, ': ', similarity)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, we apply Lesk algorithm to the sentences of the trial set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id1': [['the/DT', 'bird/NN/02', 'is/VBZ/12', 'bathing/NN/01', 'in/IN', 'the/DT', 'sink/NN/01', './.'], ['birdie/NN/01', 'is/VBZ/12', 'washing/VBG/09', 'itself/PRP', 'in/IN', 'the/DT', 'water/NN/01', 'basin/NN/01', './.']], 'id2': [['in/IN', 'may/MD', '2010/CD', ',/,', 'the/DT', 'troops/NNS/02', 'attempted/VBN/01', 'to/TO', 'invade/NN', 'kabul/NN/01', './.'], ['the/DT', 'us/PRP', 'army/NN/01', 'invaded/VBN/03', 'kabul/NN/01', 'on/IN', 'may/MD', '7th/NNS', 'last/JJ/02', 'year/NN/03', ',/,', '2010/CD', './.']], 'id3': [['john/NN/01', 'said/VBD/01', 'he/PRP', 'is/VBZ/02', 'considered/VBN/02', 'a/DT', 'witness/NN/05', 'but/CC', 'not/RB', 'a/DT', 'suspect/NN/01', './.'], ['``/``', 'he/PRP', 'is/VBZ/02', 'not/RB', 'a/DT', 'suspect/NN/01', 'anymore/RB', './.', \"''/''\", 'john/NN/01', 'said/VBD/01', './.']], 'id4': [['they/PRP', 'flew/NN', 'out/IN', 'of/IN', 'the/DT', 'nest/JJS', 'in/IN', 'groups/NNS/02', './.'], ['they/PRP', 'flew/NN', 'into/IN', 'the/DT', 'nest/JJS', 'together/RB', './.']], 'id5': [['the/DT', 'woman/NN/02', 'is/VBZ/05', 'playing/VBG/35', 'the/DT', 'violin/NN/01', './.'], ['the/DT', 'young/JJ/01', 'lady/NN/03', 'enjoys/NNS', 'listening/VBG/01', 'to/TO', 'the/DT', 'guitar/NN/01', './.']], 'id6': [['john/NN/01', 'went/VBD/04', 'horse/NN/02', 'back/RB', 'riding/VBG/13', 'at/IN', 'dawn/NN/01', 'with/IN', 'a/DT', 'whole/JJ/02', 'group/NN/02', 'of/IN', 'friends/NNS/05', './.'], ['sunrise/NN/03', 'at/IN', 'dawn/NN/03', 'is/VBZ/12', 'a/DT', 'magnificent/NN', 'view/NN/07', 'to/TO', 'take/VB/34', 'in/IN', 'if/IN', 'you/PRP', 'wake/NN/03', 'up/RB', 'early/RB', 'enough/RB', 'for/IN', 'it/PRP', './.']]}\n"
     ]
    }
   ],
   "source": [
    "new_d = compute_lesk(d)\n",
    "print(new_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now compute the Jaccard distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Jaccard similarity for each pair of sentences:\n",
      "id1 :  0.33333333333333337\n",
      "id2 :  0.33333333333333337\n",
      "id3 :  0.5714285714285714\n",
      "id4 :  0.4545454545454546\n",
      "id5 :  0.16666666666666663\n",
      "id6 :  0.09999999999999998\n"
     ]
    }
   ],
   "source": [
    "similarities = jaccard(new_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now compute the Pearson's correlation between the previously computed values and the values from the golden standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5444093003285272\n"
     ]
    }
   ],
   "source": [
    "gs = open('../trial/STS.gs.txt', 'r')\n",
    "\n",
    "for line in gs:\n",
    "    fields = line.strip().split('\\t')\n",
    "    similarities[fields[0]].append(int(fields[1]))\n",
    "\n",
    "jaccard_distances = []\n",
    "golden_record = []\n",
    "for key in similarities:\n",
    "    jaccard_distances.append(similarities[key][0])\n",
    "    golden_record.append(similarities[key][1])\n",
    "    \n",
    "print(pearsonr(golden_record, jaccard_distances)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results\n",
    "In the session 2 and 3 we got the following results:\n",
    "\n",
    "### Session 2:\n",
    "Computed Jaccard similarity for each pair of sentences:\n",
    "* 'id1' :  0.3076923076923077\n",
    "* 'id2' :  0.26315789473684215\n",
    "* 'id3' :  0.4666666666666667\n",
    "* 'id4' :  0.4545454545454546\n",
    "* 'id5' :  0.23076923076923073\n",
    "* 'id6' :  0.13793103448275867\n",
    "\n",
    "Pearson's correlation: 0.3962389776119233\n",
    "\n",
    "### Session 3:\n",
    "Computed Jaccard similarity for each pair of sentences:\n",
    "* 'id1': 0.33333333333333337\n",
    "* 'id2': 0.4117647058823529\n",
    "* 'id3': 0.5714285714285714\n",
    "* 'id4': 0.4545454545454546\n",
    "* 'id5': 0.16666666666666663\n",
    "* 'id6': 0.13793103448275867\n",
    "\n",
    "Pearson's correlation: 0.5790860088205633"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we remove the stopwords from the sentences of the trial set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id1': [['bird', 'bathing', 'sink', '.'], ['birdie', 'washing', 'water', 'basin', '.']], 'id2': [['may', '2010', ',', 'troops', 'attempted', 'invade', 'kabul', '.'], ['us', 'army', 'invaded', 'kabul', 'may', '7th', 'last', 'year', ',', '2010', '.']], 'id3': [['john', 'said', 'considered', 'witness', 'suspect', '.'], ['``', 'suspect', 'anymore', '.', \"''\", 'john', 'said', '.']], 'id4': [['flew', 'nest', 'groups', '.'], ['flew', 'nest', 'together', '.']], 'id5': [['woman', 'playing', 'violin', '.'], ['young', 'lady', 'enjoys', 'listening', 'guitar', '.']], 'id6': [['john', 'went', 'horse', 'back', 'riding', 'dawn', 'whole', 'group', 'friends', '.'], ['sunrise', 'dawn', 'magnificent', 'view', 'take', 'wake', 'early', 'enough', '.']]}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw=set(stopwords.words('english'))\n",
    "\n",
    "filtered = {}\n",
    "for key in d:\n",
    "    filtered[key] = []\n",
    "    for s in d[key]:\n",
    "        new_s = [w for w in s if w not in sw]\n",
    "        filtered[key].append(new_s)\n",
    "\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we apply Lesk algorithm to the sentences of the trial set from which we removed the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id1': [['bird/NN/02', 'bathing/NN/01', 'sink/NN/01', './.'], ['birdie/NN/01', 'washing/VBG/09', 'water/NN/02', 'basin/NN/01', './.']], 'id2': [['may/MD', '2010/CD', ',/,', 'troops/NNS/04', 'attempted/VBN/01', 'invade/NN', 'kabul/NN/01', './.'], ['us/PRP', 'army/NN/01', 'invaded/VBN/04', 'kabul/NN/01', 'may/MD', '7th/NNS', 'last/JJ/02', 'year/NN/02', ',/,', '2010/CD', './.']], 'id3': [['john/NN/03', 'said/VBD/01', 'considered/VBN/02', 'witness/NN/05', 'suspect/NN/01', './.'], ['``/``', 'suspect/NN/01', 'anymore/RB', './.', \"''/''\", 'john/NN/03', 'said/VBD/01', './.']], 'id4': [['flew/NN', 'nest/JJS', 'groups/NNS/03', './.'], ['flew/NN', 'nest/JJS', 'together/RB', './.']], 'id5': [['woman/NN/02', 'playing/VBG/02', 'violin/NN/01', './.'], ['young/JJ/01', 'lady/NN/03', 'enjoys/NNS', 'listening/VBG/02', 'guitar/NN/01', './.']], 'id6': [['john/NN/01', 'went/VBD/04', 'horse/NN/01', 'back/RB', 'riding/VBG/01', 'dawn/NN/03', 'whole/JJ/02', 'group/NN/03', 'friends/NNS/01', './.'], ['sunrise/NN/03', 'dawn/NN/03', 'magnificent/NN', 'view/NN/07', 'take/VB/24', 'wake/NN/01', 'early/RB', 'enough/RB', './.']]}\n"
     ]
    }
   ],
   "source": [
    "new_d = compute_lesk(filtered)\n",
    "print(new_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now compute the Jaccard distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Jaccard similarity for each pair of sentences:\n",
      "id1 :  0.125\n",
      "id2 :  0.3571428571428571\n",
      "id3 :  0.4444444444444444\n",
      "id4 :  0.6\n",
      "id5 :  0.11111111111111116\n",
      "id6 :  0.11764705882352944\n"
     ]
    }
   ],
   "source": [
    "similarities_filtered = jaccard(new_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now compute the Pearson's correlation between the previously computed values and the values from the golden standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16025255903367078\n"
     ]
    }
   ],
   "source": [
    "jaccard_distances = []\n",
    "for key in similarities_filtered:\n",
    "    jaccard_distances.append(similarities_filtered[key][0])\n",
    "\n",
    "print(pearsonr(golden_record, jaccard_distances)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
