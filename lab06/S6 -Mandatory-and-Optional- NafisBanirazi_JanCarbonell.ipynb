{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Exercise - Session 6\n",
    "## Students: Nafis Banirazi & Jan Carbonell\n",
    "### Lab Objective:\n",
    "The Objective of this lab are the following:\n",
    "\n",
    "To read all pairs of sentences from a trial set, compute their similarities considering Lesk's algorithm and using the Jaccard distance.\n",
    "Compare the results with those on session 2 and 3.\n",
    "Compare the results with the gold standard by giving the Pearson correlation.\n",
    "Answering if words or lemmas perfom better and if we could extrapolate that to any pair of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intial set of imports\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.wsd import lesk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "#variable initialization and instantiation\n",
    "d = {}\n",
    "tests = []\n",
    "corpus = []\n",
    "tests = []\n",
    "standard = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read all pairs of sentences of the trial set within the evaluation framework of the project.\n",
    "We proceed to open and read the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the bird is bathing in the sink.', 'birdie is washing itself in the water basin.']\n",
      "['in may 2010, the troops attempted to invade kabul.', 'the us army invaded kabul on may 7th last year, 2010.']\n",
      "['john said he is considered a witness but not a suspect.', '\"he is not a suspect anymore.\" john said.']\n",
      "['they flew out of the nest in groups.', 'they flew into the nest together.']\n",
      "['the woman is playing the violin.', 'the young lady enjoys listening to the guitar.']\n",
      "['john went horse back riding at dawn with a whole group of friends.', 'sunrise at dawn is a magnificent view to take in if you wake up early enough for it.']\n",
      "{'id1': ['the bird is bathing in the sink.', 'birdie is washing itself in the water basin.'], 'id2': ['in may 2010, the troops attempted to invade kabul.', 'the us army invaded kabul on may 7th last year, 2010.'], 'id3': ['john said he is considered a witness but not a suspect.', '\"he is not a suspect anymore.\" john said.'], 'id4': ['they flew out of the nest in groups.', 'they flew into the nest together.'], 'id5': ['the woman is playing the violin.', 'the young lady enjoys listening to the guitar.'], 'id6': ['john went horse back riding at dawn with a whole group of friends.', 'sunrise at dawn is a magnificent view to take in if you wake up early enough for it.']}\n"
     ]
    }
   ],
   "source": [
    "for line in open('./trial/STS.input.txt','r'):\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    if line[0] in d:\n",
    "        print('wtf')\n",
    "        pass\n",
    "    else:\n",
    "        d[line[0]] = [line[1].lower(), line[2].lower()] #.lower(),line[2].lower()]\n",
    "    \n",
    "    \n",
    "for key in d:\n",
    "    print(d[key])\n",
    "print(d)\n",
    "d3 = dict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply Leskâ€™s algorithm to the words in the sentences.\n",
    "In this step, we must first tokenize the sentences. \n",
    "We then apply pos_tag to determine the type of each word in the sentence; and apply lesk algorithm.\n",
    "Then we take the sets to unique values and delete None element.\n",
    "Finally, we compute the jaccard similarity as 1 - jaccard distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesking_sentence(pos_tagged_sentence):\n",
    "    \"\"\"\n",
    "    Returns a sentence as the given sentece using lesker algorithms.\n",
    "    The input sentence must be a pos_tagged sentence (e.g. [('The', 'DN'),\n",
    "    ('sun', 'NN')]).\n",
    "    \"\"\"\n",
    "    sentence = [i[0] for i in pos_tagged_sentence]\n",
    "    result = []\n",
    "    \n",
    "    none_type_objects = []\n",
    "    for word, tag in pos_tagged_sentence:\n",
    "        # 'NoneType' object has no attribute 'name'\n",
    "        try:\n",
    "            if tag.startswith('N'):\n",
    "                result.append(lesk(sentence, word, wn.NOUN).name())\n",
    "            #if word is a verb\n",
    "            elif tag.startswith('V'):\n",
    "                result.append(lesk(sentence, word, wn.VERB).name())\n",
    "            #if word is an adjective\n",
    "            elif tag.startswith('J'):\n",
    "                result.append(lesk(sentence, word, wn.ADJ).name())\n",
    "            #if word is a verb\n",
    "            elif tag.startswith('R'):\n",
    "                result.append(lesk(sentence, word, wn.ADV).name())\n",
    "            else:\n",
    "                result.append(word)\n",
    "        except:\n",
    "            print('EXEPTION ERROR:', word, tag)\n",
    "            none_type_objects.append([word, tag, lesk(sentence, word, wn.NOUN)])\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post_Lesked_Sentence:  ['the', 'bird.n.02', 'be.v.12', 'bathe.v.01', 'in', 'the', 'sinkhole.n.01', '.'] \n",
      "\n",
      "Post_Lesked_Sentence:  ['shuttlecock.n.01', 'be.v.12', 'wash.v.09', 'itself', 'in', 'the', 'body_of_water.n.01', 'washbasin.n.01', '.'] \n",
      "\n",
      "Post_Lesked_Sentence:  ['in', 'may', '2010', ',', 'the', 'troop.n.02', 'undertake.v.01', 'to', 'invade.v.04', 'kabul.n.01', '.'] \n",
      "\n",
      "EXEPTION ERROR: army VBP\n",
      "EXEPTION ERROR: invaded JJ\n",
      "Post_Lesked_Sentence:  ['the', 'us', 'kabul.n.01', 'on', 'may', '7th', 'last.a.02', 'year.n.03', ',', '2010', '.'] \n",
      "\n",
      "Post_Lesked_Sentence:  ['whoremaster.n.01', 'suppose.v.01', 'he', 'embody.v.02', 'view.v.02', 'a', 'witness.n.05', 'but', 'not.r.01', 'a', 'defendant.n.01', '.'] \n",
      "\n",
      "Post_Lesked_Sentence:  ['``', 'he', 'embody.v.02', 'not.r.01', 'a', 'defendant.n.01', 'anymore.r.01', '.', \"''\", 'whoremaster.n.01', 'suppose.v.01', '.'] \n",
      "\n",
      "EXEPTION ERROR: nest JJS\n",
      "Post_Lesked_Sentence:  ['they', 'fly.v.12', 'out', 'of', 'the', 'in', 'group.n.02', '.'] \n",
      "\n",
      "EXEPTION ERROR: nest JJS\n",
      "Post_Lesked_Sentence:  ['they', 'fly.v.10', 'into', 'the', 'together.r.04', '.'] \n",
      "\n",
      "Post_Lesked_Sentence:  ['the', 'woman.n.02', 'be.v.05', 'play.v.35', 'the', 'violin.n.01', '.'] \n",
      "\n",
      "Post_Lesked_Sentence:  ['the', 'young.a.01', 'lady.n.03', 'love.v.02', 'heed.v.01', 'to', 'the', 'guitar.n.01', '.'] \n",
      "\n",
      "Post_Lesked_Sentence:  ['toilet.n.01', 'plump.v.04', 'knight.n.02', 'back.r.02', 'ride.v.13', 'at', 'dawn.n.01', 'with', 'a', 'whole.a.02', 'group.n.02', 'of', 'friend.n.05', '.'] \n",
      "\n",
      "EXEPTION ERROR: magnificent JJ\n",
      "Post_Lesked_Sentence:  ['sunrise.n.03', 'at', 'dawn.n.03', 'be.v.12', 'a', 'view.n.07', 'to', 'take.v.34', 'in', 'if', 'you', 'awaken.v.01', 'up.r.05', 'early_on.r.01', 'enough.r.01', 'for', 'it', '.'] \n",
      "\n",
      "[0.333, 0.375, 0.571, 0.273, 0.167, 0.103]\n"
     ]
    }
   ],
   "source": [
    "for key in d:\n",
    "    \n",
    "    # Tokenizing\n",
    "    sents = [nltk.word_tokenize(s) for s in d[key]];    \n",
    "    \n",
    "    w1 = sents[0]\n",
    "    w2 = sents[1]\n",
    "\n",
    "    # Determining the category of words\n",
    "    wordtype1 = pos_tag(w1)    \n",
    "    wordtype2 = pos_tag(w2)\n",
    "\n",
    "    # As Lesk's algorithm works with synsets\n",
    "    # Not every word will be accepted. Just nouns, verbs, adjectives and adverbs\n",
    "    #determining the definition of words\n",
    "    lesk1 = lesking_sentence(wordtype1)\n",
    "    print('Post_Lesked_Sentence: ', lesk1, '\\n')\n",
    "    lesk2 = lesking_sentence(wordtype2)\n",
    "    print('Post_Lesked_Sentence: ', lesk2, '\\n')\n",
    "\n",
    "    # Removing duplicates\n",
    "    set1 = set(lesk1)\n",
    "    set2 = set(lesk2)\n",
    "    \n",
    "    #computing jaccard similarity\n",
    "    jaccard_similarity = 1 - jaccard_distance(set1, set2)\n",
    "    tests.append(round(jaccard_similarity,3))\n",
    "print(tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute their similarities via the Jaccard coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done previously within the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare the results with those in session 2 (document) and 3 (morphology) in which words and lemmas were considered.\n",
    "The gold standard results would be 5,4,3,2,1,0.\n",
    "\n",
    "\n",
    "In session 2 we got the following results:\n",
    "\n",
    "* *'id1': 0.308* --> 1.54\n",
    "* *'id2': 0.263* --> 1.31\n",
    "* *'id3': 0.467* --> 2.34\n",
    "* *'id4': 0.455* --> 2.27\n",
    "* *'id5': 0.231* --> 1.15\n",
    "* *'id6': 0.138* --> 0.69\n",
    "\n",
    "In session 3, using lemmas similarity, we obtained:\n",
    "* *'id1': 0.333* --> 1.67\n",
    "* *'id2': 0.412* --> 2.06\n",
    "* *'id3': 0.571* --> 2.86\n",
    "* *'id4': 0.333* --> 1.67\n",
    "* *'id5': 0.167* --> 0.84\n",
    "* *'id6': 0.138* --> 0.69\n",
    "\n",
    "And our current result is:\n",
    "* *'id1': 0.333* --> 1.67\n",
    "* *'id2': 0.375* --> 1.88\n",
    "* *'id3': 0.571* --> 2.86\n",
    "* *'id4': 0.273* --> 1.36\n",
    "* *'id5': 0.167* --> 0.84\n",
    "* *'id6': 0.103* --> 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare the results with gold standard by giving the pearson correlation between them.\n",
    "And now, we open the Golden Standard file and calculate the perason correlation with lesk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation with lesk: 0.668\n"
     ]
    }
   ],
   "source": [
    "# CAREFUL WITH THE STS.gs FILE. \n",
    "# NEEDS TO BE CORRECTED TO 54321 instead of 12345\n",
    "\n",
    "for line in open('./trial/STS.gs.txt','r'):\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    standard.append(int(line[1]))\n",
    "    \n",
    "a = pearsonr(standard[0:6], tests[0:6])[0]\n",
    "print('Pearson correlation with lesk:', round(a,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the results with those in session 2 (document) and 3 (morphology) in which words and lemmas were considered:\n",
    "- Pearson correlation without lemmatization: 0.396\n",
    "- Pearson correlation with lemmatization: 0.578\n",
    "- Pearson correlation with lesk: 0.668\n",
    "\n",
    "This may also be to an improvement in our way of splitting the words and sorting the sentences that we have implemented in this lab. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercise\n",
    "### Stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'id1': ['the bird is bathing in the sink.', 'birdie is washing itself in the water basin.'], 'id2': ['in may 2010, the troops attempted to invade kabul.', 'the us army invaded kabul on may 7th last year, 2010.'], 'id3': ['john said he is considered a witness but not a suspect.', '\"he is not a suspect anymore.\" john said.'], 'id4': ['they flew out of the nest in groups.', 'they flew into the nest together.'], 'id5': ['the woman is playing the violin.', 'the young lady enjoys listening to the guitar.'], 'id6': ['john went horse back riding at dawn with a whole group of friends.', 'sunrise at dawn is a magnificent view to take in if you wake up early enough for it.']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We apply lesk and compute the jaccard distance and its corresponding pearson coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post_Lesked_Sentence:  ['bird.n.02', 'bathe.v.03', 'sinkhole.n.01', '.'] \n",
      "\n",
      "Post_Lesked_Sentence:  ['shuttlecock.n.01', 'wash.v.09', 'water_system.n.02', 'washbasin.n.01', '.'] \n",
      "\n",
      "EXEPTION ERROR: invade JJ\n",
      "Post_Lesked_Sentence:  ['may', '2010', ',', 'troop.n.04', 'undertake.v.01', 'kabul.n.01', '.'] \n",
      "\n",
      "EXEPTION ERROR: army VBP\n",
      "EXEPTION ERROR: invaded JJ\n",
      "Post_Lesked_Sentence:  ['us', 'kabul.n.01', 'may', '7th', 'last.a.02', 'year.n.02', ',', '2010', '.'] \n",
      "\n",
      "EXEPTION ERROR: witness JJ\n",
      "Post_Lesked_Sentence:  ['john.n.03', 'suppose.v.01', 'view.v.02', 'suspect.n.01', '.'] \n",
      "\n",
      "Post_Lesked_Sentence:  ['``', 'suspect.n.01', 'anymore.r.01', '.', \"''\", 'john.n.03', 'suppose.v.01', '.'] \n",
      "\n",
      "EXEPTION ERROR: flew JJ\n",
      "EXEPTION ERROR: nest JJS\n",
      "Post_Lesked_Sentence:  ['group.n.03', '.'] \n",
      "\n",
      "EXEPTION ERROR: flew JJ\n",
      "EXEPTION ERROR: nest JJS\n",
      "Post_Lesked_Sentence:  ['together.r.05', '.'] \n",
      "\n",
      "Post_Lesked_Sentence:  ['womanhood.n.02', 'toy.v.02', 'violin.n.01', '.'] \n",
      "\n",
      "Post_Lesked_Sentence:  ['young.a.01', 'lady.n.03', 'love.v.02', 'listen.v.02', 'guitar.n.01', '.'] \n",
      "\n",
      "Post_Lesked_Sentence:  ['whoremaster.n.01', 'plump.v.04', 'sawhorse.n.01', 'back.r.02', 'ride.v.01', 'dawn.n.03', 'whole.a.02', 'group.n.03', 'supporter.n.01', '.'] \n",
      "\n",
      "EXEPTION ERROR: magnificent JJ\n",
      "Post_Lesked_Sentence:  ['sunrise.n.03', 'dawn.n.03', 'view.n.07', 'take.v.24', 'wake_island.n.01', 'early_on.r.01', 'enough.r.01', '.'] \n",
      "\n",
      "[0.333, 0.375, 0.571, 0.273, 0.167, 0.103]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "without_stop_words_tests = []\n",
    "\n",
    "def words_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "sw=set(stopwords.words('english'))\n",
    "for key in d3:\n",
    "    \n",
    "    # Tokenizing\n",
    "    #sent = words_from_sent(d3[key]);    \n",
    "    \n",
    "    \n",
    "    w1 = words_from_sent(d3[key][0])\n",
    "    w2 = words_from_sent(d3[key][1])\n",
    "\n",
    "    # Determining the category of words\n",
    "    wordtype1 = pos_tag(w1)    \n",
    "    wordtype2 = pos_tag(w2)\n",
    "\n",
    "    # As Lesk's algorithm works with synsets\n",
    "    # Not every word will be accepted. Just nouns, verbs, adjectives and adverbs\n",
    "    #determining the definition of words\n",
    "    lesk1 = lesking_sentence(wordtype1)\n",
    "    print('Post_Lesked_Sentence: ', lesk1, '\\n')\n",
    "    lesk2 = lesking_sentence(wordtype2)\n",
    "    print('Post_Lesked_Sentence: ', lesk2, '\\n')\n",
    "\n",
    "    # Removing duplicates\n",
    "    set1 = set(lesk1)\n",
    "    set2 = set(lesk2)\n",
    "    \n",
    "    #computing jaccard similarity\n",
    "    jaccard_similarity = 1 - jaccard_distance(set1, set2)\n",
    "    without_stop_words_tests.append(round(jaccard_similarity,3))\n",
    "print(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation with lesk: 0.36\n"
     ]
    }
   ],
   "source": [
    "# CAREFUL WITH THE STS.gs FILE. \n",
    "# NEEDS TO BE CORRECTED TO 54321 instead of 12345\n",
    "\n",
    "for line in open('./trial/STS.gs.txt','r'):\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    standard.append(int(line[1]))\n",
    "    \n",
    "a = pearsonr(standard[0:6], without_stop_words_tests[0:6])[0]\n",
    "print('Pearson correlation with lesk:', round(a,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, the pearson coefficient is reduced by a great amount once we take away the stopwords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
