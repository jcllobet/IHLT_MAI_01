{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mandatory Exercise - Session 6\n",
    "Students: Nafis Banirazi & Jan Carbonell\n",
    "Lab Objective:\n",
    "The Objective of this lab are the following:\n",
    "\n",
    "To read all pairs of sentences from a trial set, compute their similarities considering Lesk's algorithm and using the Jaccard distance.\n",
    "Compare the results with those on session 2 and 3.\n",
    "Compare the results with the gold standard by giving the Pearson correlation.\n",
    "Answering if words or lemmas perfom better and if we could extrapolate that to any pair of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intial set of imports\n",
    "import nltk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk import pos_tag\n",
    "from nltk.wsd import lesk\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "#variable initialization and instantiation\n",
    "corpus = []\n",
    "tests = []\n",
    "standard = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to open and read the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open('STS.input.txt','r'):\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    corpus.append(line[1:])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the Jaccard distance.\n",
    "In this step, we must first tokenize the sentences. \n",
    "We then apply pos_tag to determine the type of each word in the sentence; and apply lesk algorithm.\n",
    "Then we take the sets to unique values and delete None element.\n",
    "Finally, we compute the jaccard similarity as 1 - jaccard distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Synset('be.v.12'), Synset('bathe.v.01'), Synset('bird.n.02'), Synset('sinkhole.n.01')}\n",
      "{Synset('be.v.12'), Synset('body_of_water.n.01'), Synset('washbasin.n.01'), Synset('wash.v.09'), Synset('shuttlecock.n.01')}\n",
      "0.125\n",
      "{Synset('kabul.n.01'), Synset('whitethorn.n.01'), Synset('troop.n.02'), Synset('invade.v.01'), Synset('undertake.v.01')}\n",
      "{Synset('uranium.n.01'), Synset('kabul.n.01'), Synset('year.n.02'), Synset('whitethorn.n.01'), Synset('invade.v.03'), Synset('united_states_army.n.01')}\n",
      "0.2222222222222222\n",
      "{Synset('embody.v.02'), Synset('whoremaster.n.01'), Synset('witness.n.05'), Synset('not.r.01'), Synset('view.v.02'), Synset('defendant.n.01'), Synset('suppose.v.01')}\n",
      "{Synset('embody.v.02'), Synset('whoremaster.n.01'), Synset('not.r.01'), Synset('defendant.n.01'), Synset('anymore.r.01'), Synset('suppose.v.01')}\n",
      "0.625\n",
      "{Synset('fly.v.12'), Synset('group.n.02')}\n",
      "{Synset('fly.v.10'), Synset('together.r.04')}\n",
      "0.0\n",
      "{Synset('woman.n.02'), Synset('play.v.35'), Synset('be.v.05'), Synset('violin.n.01')}\n",
      "{Synset('love.v.02'), Synset('guitar.n.01'), Synset('lady.n.03'), Synset('heed.v.01')}\n",
      "0.0\n",
      "{Synset('plump.v.04'), Synset('group.n.02'), Synset('knight.n.02'), Synset('dawn.n.01'), Synset('back.r.02'), Synset('friend.n.05'), Synset('ride.v.13'), Synset('toilet.n.01')}\n",
      "{Synset('be.v.12'), Synset('enough.r.01'), Synset('dawn.n.03'), Synset('awaken.v.01'), Synset('up.r.05'), Synset('take.v.34'), Synset('early_on.r.01'), Synset('sunrise.n.03'), Synset('view.n.07')}\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for pair in corpus:\n",
    "    \n",
    "    #tokenizing\n",
    "    sents = [nltk.word_tokenize(s) for s in pair];    \n",
    "    w1 = sents[0]\n",
    "    w2 = sents[1]\n",
    "\n",
    "    #determining the category of words\n",
    "    wordtype1 = pos_tag(w1)    \n",
    "    wordtype2 = pos_tag(w2)\n",
    "\n",
    "    #determining the definition of words\n",
    "    lesk1 = [lesk(w1, t[0], t[1][0].lower()) for t in wordtype1]\n",
    "    lesk2 = [lesk(w2, t[0], t[1][0].lower()) for t in wordtype2]\n",
    "\n",
    "    #removing duplicates and None elements\n",
    "    set1 = set(lesk1)\n",
    "    set1.remove(None)\n",
    "    set2 = set(lesk2)\n",
    "    set2.remove(None)\n",
    "    \n",
    "    #computing jaccard similarity\n",
    "    jaccard_similarity = 1 - jaccard_distance(set1, set2)\n",
    "    tests.append(jaccard_similarity)\n",
    "    print(set1)\n",
    "    print(set2)\n",
    "    print(jaccard_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we open the Golden Standard file and calculate the perason correlation with and without lesk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation with lesk: -0.42\n"
     ]
    }
   ],
   "source": [
    "for line in open('STS.gs.txt','r'):\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    standard.append(int(line[1]))\n",
    "    \n",
    "a = pearsonr(standard[0:6], tests[0:6])[0]\n",
    "print('Pearson correlation with lesk:', round(a,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the results with those in session 2 (document) and 3 (morphology) in which words and lemmas were considered:\n",
    "Pearson correlation without lemmatization: 0.396\n",
    "Pearson correlation with lemmatization: 0.578\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
