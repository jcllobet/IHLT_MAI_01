{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.metrics import jaccard_distance\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read all pairs of sentences of the trial set within the evaluation framework of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will be reading the trial set, which is located at ../trial/STS.input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../trial/STS.input.txt','r') as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the raw text, it is better to convert it into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id1',\n",
       "  'the bird is bathing in the sink.',\n",
       "  'birdie is washing itself in the water basin.'],\n",
       " ['id2',\n",
       "  'in may 2010, the troops attempted to invade kabul.',\n",
       "  'the us army invaded kabul on may 7th last year, 2010.'],\n",
       " ['id3',\n",
       "  'john said he is considered a witness but not a suspect.',\n",
       "  '\"he is not a suspect anymore.\" john said.'],\n",
       " ['id4',\n",
       "  'they flew out of the nest in groups.',\n",
       "  'they flew into the nest together.'],\n",
       " ['id5',\n",
       "  'the woman is playing the violin.',\n",
       "  'the young lady enjoys listening to the guitar.'],\n",
       " ['id6',\n",
       "  'john went horse back riding at dawn with a whole group of friends.',\n",
       "  'sunrise at dawn is a magnificent view to take in if you wake up early enough for it.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First we will separate each line of the text\n",
    "formatted_ids = raw_text.split(sep='\\n')\n",
    "#Removing the final blank\n",
    "formatted_ids.remove('')\n",
    "\n",
    "#Then we are going to split once again each line to get \n",
    "#the sentences we need\n",
    "formatted_words = [lst.lower().split(sep='\\t') for lst in formatted_ids]\n",
    "formatted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply Leskâ€™s algorithm to the words in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'formatted_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-59a69acb3f5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#First we need the words:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m test_words = [[formatted_words[i][0],nltk.word_tokenize(formatted_words[i][1]),\n\u001b[0;32m----> 3\u001b[0;31m                nltk.word_tokenize(formatted_words[i][2])] for i in range(len(formatted_words))]\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#Then their POS:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'formatted_words' is not defined"
     ]
    }
   ],
   "source": [
    "#First we need the words:\n",
    "test_words = [[formatted_words[i][0],nltk.word_tokenize(formatted_words[i][1]),\n",
    "               nltk.word_tokenize(formatted_words[i][2])] for i in range(len(formatted_words))]\n",
    "#Then their POS:\n",
    "test_pos = [[i[0], pos_tag(i[1]), pos_tag(i[2])] for i in test_words]\n",
    "print(test_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Lesk's algorithm works with synsets, not every word will be accepted. Just nouns, verbs, adjectives and adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pairs = []\n",
    "for i in range(len(test_pos)):\n",
    "    valid_pairs.append([])\n",
    "    valid_pairs[i].append(test_pos[i][0])\n",
    "    for j in range(2):\n",
    "        valid_pairs[i].append([])\n",
    "        valid_pairs[i][j+1].extend(lesker_sentence(test_pos[i][j+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesker_sentence(pos_tag_sentence):\n",
    "    \"\"\"\n",
    "    Returns a sentence as the given sentece using lesker algorithms.\n",
    "    The input sentence must be a pos_tagged sentence (e.g. [('The', 'DN'),\n",
    "    ('sun', 'NN')]).\n",
    "    \"\"\"\n",
    "    sentence = [i[0] for i in pos_tag_sentence]\n",
    "    final_sentence = []\n",
    "    for word, tag in pos_tag_sentence:\n",
    "        #if word is a noun\n",
    "        if tag.startswith('N') and (type(lesk(sentence, word, wn.NOUN))!=type(None)):\n",
    "            final_sentence.append(lesk(sentence, word, wn.NOUN).name())\n",
    "        #if word is a verb\n",
    "        elif tag.startswith('V') and (type(lesk(sentence, word, wn.VERB))!=type(None)):\n",
    "            final_sentence.append(lesk(sentence, word, wn.VERB).name())\n",
    "        #if word is an adjective\n",
    "        elif tag.startswith('J') and (type(lesk(sentence, word, wn.ADJ))!=type(None)):\n",
    "            final_sentence.append(lesk(sentence, word, wn.ADJ).name())\n",
    "        #if word is a verb\n",
    "        elif tag.startswith('R') and (type(lesk(sentence, word, wn.ADV))!=type(None)):\n",
    "            final_sentence.append(lesk(sentence, word, wn.ADV).name())\n",
    "        else:\n",
    "            final_sentence.append(word)\n",
    "    return final_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute their similarities by considering senses and Jaccard coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we hace converted all our synset words considering their most probable sense, we are able to compute their similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.5714285714285714,\n",
       " 0.33333333333333337,\n",
       " 0.16666666666666663,\n",
       " 0.09999999999999998]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = [1.-jaccard_distance(set(i[1]), set(i[2])) for i in valid_pairs]\n",
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare the results with those in session 2 (document) and 3 (morphology) in which words and lemmas were considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In session 2 we got the following results inwhich raw words were considered:\n",
    "* **'id1': 0.3076923076923077**\n",
    "* **'id2': 0.26315789473684215**\n",
    "* **'id3': 0.4666666666666667**\n",
    "* **'id4': 0.4545454545454546**\n",
    "* **'id5': 0.23076923076923073**\n",
    "* **'id6': 0.13793103448275867**\n",
    "\n",
    "In ession 3, using lemma similarity, we got:\n",
    "* **'id1': 0.33333333333333337**\n",
    "* **'id2': 0.4117647058823529**\n",
    "* **'id3': 0.5714285714285714**\n",
    "* **'id4': 0.4545454545454546**\n",
    "* **'id5': 0.16666666666666663**\n",
    "* **'id6': 0.13793103448275867**\n",
    "\n",
    "As we can see, using synsets the similarities have changed significantly. The main changes that we see are:\n",
    "* Same similarity in the first pair: this is due to the fact that although considering synsets, the results weren't the same synsets for the words considered. If two words don't have the same sense, this won't count as a similarity.\n",
    "* Lower similarity on the last pair: using word sense disambiguation has made that this two sentences have became less similar. This is due to the fact that although having a same word, their senses are not the same, and so they are not similar\n",
    "\n",
    "Tere have been also some minor changes in the second pair, in which similarity has decreased. Finally, the fourth part has been labeled more properly, decreasing its significance also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare the results with gold standard by giving the pearson correlation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading he gold standard:\n",
    "with open('../trial/STS.gs.txt','r') as f:\n",
    "    plain_gs = f.read()\n",
    "\n",
    "#First we will separate each line of the text\n",
    "formatted_gs = plain_gs.split(sep='\\n')\n",
    "#Removing the final blank\n",
    "formatted_gs.remove('')\n",
    "\n",
    "words_gs = [lst.split(sep='\\t') for lst in formatted_gs]\n",
    "gold_standard = [int(i[1]) for i in words_gs]\n",
    "gs_list = list(gold_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6206712050540985"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(gs_list, similarities)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the correlation with the gold standard has increased significantly compared to the previous results (0.579 the previous better one). Using word disambiguation seems a good aproach towards semantic textual similarity. Although that, this is not good enough, this process keeps asigning a low similarity to the first pair. \n",
    "\n",
    "In order to increase the similarity of those sentences, we could use more information from the synsets that we have just extracted. Comparing the distances in the synset graph could give us more information in order to get a better similarity. FOr instance, two words could be similar, and they might have different synsets. But those synsets would be similar if those words are clo though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
