{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Exercise - Session 4\n",
    "\n",
    "### Students: Jan Carbonell & Nafis\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this lab is to train, test and benchmark the outcome of different Part of Speech (POS) models against the threebank corpus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports. Could also be done in the pc environment\n",
    "import nltk\n",
    "import dill\n",
    "from nltk.corpus import treebank\n",
    "import timeit\n",
    "\n",
    "# additional dependencies from each model\n",
    "from nltk.tag import tnt\n",
    "from nltk.tag import CRFTagger\n",
    "from nltk.tag.hmm import HiddenMarkovModelTrainer\n",
    "from nltk.tag.perceptron import PerceptronTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the initial conditions stated in the problem, starting the dictionary with each of the model tags, create an array with the stopping points of the training set and the test data from sentences 3001 and onwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stop = [500, 1000, 1500, 2000, 2500, 3000]\n",
    "test_data = treebank.tagged_sents()[3001:]\n",
    "\n",
    "accuracy = {'HMM':[], 'TnT':[], 'PER':[], 'CRF':[]}\n",
    "time = {'HMM':[], 'TnT':[], 'PER':[], 'CRF':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the different models and add each of the test results to the dictionary. The reason why we want to evaluate the performance of the test data is to measure how well the model behaves against an unkown set of words. If we were to compare against the training data, with each step its accuary would increase -eventually reaching 100%- but this wouldn't necessarly mean that it is also suitable for unknown words, as the solutions would tend to overfit as we feed them more and more data. \n",
    "\n",
    "Print statements are implemented inside and at the end of the loop to ensure the correct functioning of the snippet and integrity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 500 sentences\n",
      "Training with 1000 sentences\n",
      "Training with 1500 sentences\n"
     ]
    }
   ],
   "source": [
    "for e in train_stop:\n",
    "    #reviewing that we are on the right track\n",
    "    print('Training with {} sentences'.format(e))\n",
    "    \n",
    "    train_data = treebank.tagged_sents()[:e]\n",
    "    \n",
    "    #HMM\n",
    "    start = timeit.default_timer()\n",
    "    trainer = HiddenMarkovModelTrainer()\n",
    "    HMM = trainer.train_supervised(train_data)\n",
    "    accuracy['HMM'] += [round(HMM.evaluate(test_data), 3)]\n",
    "    stop = timeit.default_timer()\n",
    "    accuracy['HMM'] += [stop - start]\n",
    "    \n",
    "    #TNT\n",
    "    start = timeit.default_timer()\n",
    "    TnT = tnt.TnT()\n",
    "    TnT.train(train_data)\n",
    "    accuracy['TnT'] += [round(TnT.evaluate(test_data), 3)]\n",
    "    stop = timeit.default_timer()\n",
    "    accuracy['TnT'] += [stop - start]\n",
    "        \n",
    "    #PER\n",
    "    start = timeit.default_timer()\n",
    "    PER = PerceptronTagger(load=False)\n",
    "    PER.train(train_data)\n",
    "    accuracy['PER'] += [round(PER.evaluate(test_data), 3)]\n",
    "    stop = timeit.default_timer()\n",
    "    accuracy['PER'] += [stop - start]\n",
    "    \n",
    "    #CRF\n",
    "    start = timeit.default_timer()\n",
    "    CRF = CRFTagger()\n",
    "    CRF.train(train_data, 'crf_tagger_model')\n",
    "    accuracy['CRF'] += [round(CRF.evaluate(test_data), 3)]\n",
    "    stop = timeit.default_timer()\n",
    "    accuracy['CRF'] += [stop - start]\n",
    "    \n",
    "#verification that it is properly stored\n",
    "for keys,values in accuracy.items():\n",
    "    print(keys, values)\n",
    "    \n",
    "for keys,values in time.items():\n",
    "    print(keys, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot the results, not only to better visualize the specific x and y coordinates that we have obtained and store but also to connect the dots and evaluate the overall increase in performance and benchmarking against the other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the necessary packages for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#preparing and plotting  the data\n",
    "plt.figure()\n",
    "x = train_stop\n",
    "plt.plot(x, accuracy['HMM'], label='HMM')\n",
    "plt.plot(x, accuracy['TnT'], label='TnT')\n",
    "plt.plot(x, accuracy['PER'], label='PER')\n",
    "plt.plot(x, accuracy['CRF'], label='CRF')\n",
    "\n",
    "#adding the legend showing the plot\n",
    "plt.xlabel('Number of Sentences')\n",
    "plt.ylabel('Model Accuracy')\n",
    "plt.title('Part Of Speech Models')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "As we can see, both the performance of CRF and PER\n",
    "\n",
    "<b>Which model would you select?</b> Justify the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
