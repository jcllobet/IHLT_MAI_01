{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Exercise - Session 4\n",
    "\n",
    "### Students: Nafis Banirazi & Jan Carbonell\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this lab is to train, test and benchmark the outcome of different Part of Speech (POS) models against the threebank corpus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports. Could also be done in the pc environment\n",
    "import nltk\n",
    "import dill\n",
    "from nltk.corpus import treebank\n",
    "import timeit\n",
    "\n",
    "# additional dependencies from each model\n",
    "from nltk.tag import tnt\n",
    "from nltk.tag import CRFTagger\n",
    "from nltk.tag.hmm import HiddenMarkovModelTrainer\n",
    "from nltk.tag.perceptron import PerceptronTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the initial conditions stated in the problem, starting the dictionary with each of the model tags, create an array with the stopping points of the training set and the test data from sentences 3001 and onwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stop = [500, 1000, 1500, 2000, 2500, 3000]\n",
    "test_data = treebank.tagged_sents()[3001:]\n",
    "\n",
    "accuracy = {'HMM':[], 'TnT':[], 'PER':[], 'CRF':[]}\n",
    "time = {'HMM':[], 'TnT':[], 'PER':[], 'CRF':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the different models and add each of the test results to the dictionary. The reason why we want to evaluate the performance of the test data is to measure how well the model behaves against an unkown set of words. If we were to compare against the training data, with each step its accuary would increase -eventually reaching 100%- but this wouldn't necessarly mean that it is also suitable for unknown words, as the solutions would tend to overfit as we feed them more and more data. \n",
    "\n",
    "Print statements are implemented inside and at the end of the loop to ensure the correct functioning of the snippet and integrity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 500 sentences\n",
      "Training with 1000 sentences\n",
      "Training with 1500 sentences\n",
      "Training with 2000 sentences\n",
      "Training with 2500 sentences\n"
     ]
    }
   ],
   "source": [
    "for e in train_stop:\n",
    "    #reviewing that we are on the right track\n",
    "    print('Training with {} sentences'.format(e))\n",
    "    \n",
    "    train_data = treebank.tagged_sents()[:e]\n",
    "    \n",
    "    #HMM\n",
    "    start = timeit.default_timer()\n",
    "    trainer = HiddenMarkovModelTrainer()\n",
    "    HMM = trainer.train_supervised(train_data)\n",
    "    accuracy['HMM'] += [round(HMM.evaluate(test_data), 3)]\n",
    "    stop = timeit.default_timer()\n",
    "    time['HMM'] += [round(stop - start, 3)]\n",
    "    \n",
    "    #TNT\n",
    "    start = timeit.default_timer()\n",
    "    TnT = tnt.TnT()\n",
    "    TnT.train(train_data)\n",
    "    accuracy['TnT'] += [round(TnT.evaluate(test_data), 3)]\n",
    "    stop = timeit.default_timer()\n",
    "    time['TnT'] += [round(stop - start, 3)]\n",
    "        \n",
    "    #PER\n",
    "    start = timeit.default_timer()\n",
    "    PER = PerceptronTagger(load=False)\n",
    "    PER.train(train_data)\n",
    "    accuracy['PER'] += [round(PER.evaluate(test_data), 3)]\n",
    "    stop = timeit.default_timer()\n",
    "    time['PER'] += [round(stop - start, 3)]\n",
    "    \n",
    "    #CRF\n",
    "    start = timeit.default_timer()\n",
    "    CRF = CRFTagger()\n",
    "    CRF.train(train_data, 'crf_tagger_model')\n",
    "    accuracy['CRF'] += [round(CRF.evaluate(test_data), 3)]\n",
    "    stop = timeit.default_timer()\n",
    "    time['CRF'] += [round(stop - start, 3)]\n",
    "    \n",
    "#verification that it is properly stored\n",
    "for keys,values in accuracy.items():\n",
    "    print(keys, values)\n",
    "    \n",
    "for keys,values in time.items():\n",
    "    print(keys, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot the results, not only to better visualize the specific x and y coordinates that we have obtained and store but also to connect the dots and evaluate the overall increase in performance and benchmarking against the other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the necessary packages for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#preparing and plotting the accuracy graph\n",
    "x = train_stop\n",
    "plt.figure()\n",
    "plt.plot(x, accuracy['HMM'], label='HMM')\n",
    "plt.plot(x, accuracy['TnT'], label='TnT')\n",
    "plt.plot(x, accuracy['PER'], label='PER')\n",
    "plt.plot(x, accuracy['CRF'], label='CRF')\n",
    "\n",
    "#adding the legend showing the plot\n",
    "plt.xlabel('Number of Sentences')\n",
    "plt.ylabel('Model Accuracy')\n",
    "plt.title('Part Of Speech Models')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Which model would you select?</b> Justify the answer.\n",
    "\n",
    "Based on the initial graph, both CRF and PER seem like the better performing algorithms. If we could only make a decision based on this data, we would pick the Perceptron model.\n",
    "\n",
    "In order to pick the best overall model, another relevant measure is the speed of the system. Because of this, we also have implemented a timer in between each of the models and will plot it accordingly to figure out which algorithms were more effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing and plotting the accuracy graph\n",
    "x = train_stop\n",
    "plt.figure()\n",
    "plt.plot(x, time['HMM'], label='HMM')\n",
    "plt.plot(x, time['TnT'], label='TnT')\n",
    "plt.plot(x, time['PER'], label='PER')\n",
    "plt.plot(x, time['CRF'], label='CRF')\n",
    "\n",
    "#adding the legend showing the plot\n",
    "plt.xlabel('Number of Sentences')\n",
    "plt.ylabel('Model Execution Time')\n",
    "plt.title('Part Of Speech Models')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, PER seems to be the most effective -accuracy wise- and second most time efficient algorithm. After having performed this additional execution time analysis <b> we are reinsured in our conclusion to pick the Perceptron Model.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Over the development of this lab, we have implemented 4 different POS Models, tested them with different segments of the treebank corpus and trained them with another set of segments. From those results, we have plotted their performanced based on accuracy and execution time and have selected the best performing model for this specific case, which happened to be the Perceptron Model. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
