{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Human Language Technology - Lab. Session 7: Word Sequences\n",
    "### Authors: Rueben Álvarez and Albert Espín"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sentence pairs, word tokenization and named-entity tagging.\n",
    "The following cell finds the pairs of sentences in the file \"STS.input.txt\" of the trial corpus. Afterwards, it stores the pairs as tuples (i.e. each sentence of the pair becomes a tuple element), with each sentence transformed into a set of tagged words and named entities obtained with word tokenization and named-entity tagging using the NLTK named-entity tagger and chunker. A function to tag and chunk using Stanford NLP named-entity tagger has also been defined, but not used for the later calculations since it keeps terms of multi-word named-entites in separate strings, while the NLTK tagger groups them as we want. Note that using current NLTK version 3.3., the API syntax is different from the statement, but the result is the same. The sentences are not turned into lower case since that would make named-entity tagger fail to identify named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "import nltk.corpus\n",
    "from nltk.metrics import jaccard_distance\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.parse import CoreNLPParser\n",
    "\n",
    "# Stanford named-entity tagger\n",
    "named_entity_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Smith', 'and', '1994', 'per', 'Google', 'Mark', 'week', 'working', 'for', '$', 'at', 'since', '1000', 'Pedersen', 'John', 'are']\n"
     ]
    }
   ],
   "source": [
    "def get_stanford_named_entity_chunked_sentence(sentence):\n",
    "    \"\"\"Given the passed sentence string, returns an array with the chunks (words and named entities) it contains, using Stanford NLP\"\"\"\n",
    "        \n",
    "    # obtain an array with the sentence tokens\n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # tag each word to classify it as a normal word or a named entity (e.g. a person or an organization)\n",
    "    tagged_sentence = set(named_entity_tagger.tag(tokenized_sentence))\n",
    "    \n",
    "    chunked_sentence = []\n",
    "    for tagged_token in tagged_sentence:\n",
    "        \n",
    "        token = tagged_token[0]\n",
    "        tag = tagged_token[1]\n",
    "        \n",
    "        # make normal words have lower case, also discard punctuation marks\n",
    "        if tag == 'O':\n",
    "            if token.isalnum():\n",
    "                chunked_sentence.append(token.lower())\n",
    "         \n",
    "        # keep named entities with the original capitalization\n",
    "        else:\n",
    "            chunked_sentence.append(token)\n",
    "    \n",
    "    return chunked_sentence\n",
    "\n",
    "# example: note that Stanford NLP naming-entity tagger does not group the terms of named entities in a single string\n",
    "print(get_stanford_named_entity_chunked_sentence(\"Mark Pedersen and John Smith are working at Google since 1994 for 1000$ per week.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and', 'are', 'working', 'Google', '1000', 'per', 'Mark Pedersen', '1994', 'since', 'at', 'week', 'for', 'John Smith'}\n"
     ]
    }
   ],
   "source": [
    "def get_named_entity_chunked_sentence(sentence):\n",
    "    \"\"\"Given the passed sentence string, returns an array with the chunks (words and named entities) it contains, using NLTK\"\"\"\n",
    "        \n",
    "    # obtain an array with the sentence tokens\n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # chunk and tag with NLTK named-entity tagged\n",
    "    chunk_tree = ne_chunk(pos_tag(word_tokenize(sentence)), binary=True)\n",
    "    \n",
    "    chunked_sentence = []\n",
    "    for chunk in chunk_tree:\n",
    "        \n",
    "        # keep named entities with the original capitalization\n",
    "        if hasattr(chunk, 'label'):\n",
    "            token = ' '.join(term[0] for term in chunk)\n",
    "            chunked_sentence.append(token)\n",
    "            \n",
    "        # make normal words have lower case, also discard puntuaction marks\n",
    "        else:\n",
    "            token = chunk[0]\n",
    "            if token.isalnum():\n",
    "                chunked_sentence.append(token.lower())\n",
    "    \n",
    "    return set(chunked_sentence)\n",
    "\n",
    "# example: note that NLTK naming-entity tagger does group the terms of named entities in a single string\n",
    "print(get_named_entity_chunked_sentence(\"Mark Pedersen and John Smith are working at Google since 1994 for 1000$ per week.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence: \t The bird is bathing in the sink. \n",
      "Second sentence: \t Birdie is washing itself in the water basin.\n",
      " \n",
      "\n",
      "First sentence: \t In May 2010, the troops attempted to invade Kabul. \n",
      "Second sentence: \t The US army invaded Kabul on May 7th last year, 2010.\n",
      " \n",
      "\n",
      "First sentence: \t John said he is considered a witness but not a suspect. \n",
      "Second sentence: \t \"He is not a suspect anymore.\" John said.\n",
      " \n",
      "\n",
      "First sentence: \t They flew out of the nest in groups. \n",
      "Second sentence: \t They flew into the nest together.\n",
      " \n",
      "\n",
      "First sentence: \t The woman is playing the violin. \n",
      "Second sentence: \t The young lady enjoys listening to the guitar.\n",
      " \n",
      "\n",
      "First sentence: \t John went horse back riding at dawn with a whole group of friends. \n",
      "Second sentence: \t Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.\n",
      " \n",
      "\n",
      "\n",
      "{'is', 'bathing', 'in', 'sink', 'bird', 'the'}\n",
      "{'basin', 'itself', 'is', 'washing', 'in', 'Birdie', 'water', 'the'}\n",
      "\n",
      "\n",
      "{'to', 'invade', 'may', 'in', 'attempted', '2010', 'troops', 'Kabul', 'the'}\n",
      "{'invaded', 'last', 'may', '7th', '2010', 'year', 'US', 'on', 'army', 'Kabul', 'the'}\n",
      "\n",
      "\n",
      "{'not', 'is', 'a', 'he', 'considered', 'witness', 'said', 'suspect', 'John', 'but'}\n",
      "{'not', 'is', 'a', 'he', 'anymore', 'said', 'suspect', 'John'}\n",
      "\n",
      "\n",
      "{'of', 'they', 'out', 'groups', 'in', 'flew', 'nest', 'the'}\n",
      "{'they', 'into', 'flew', 'nest', 'together', 'the'}\n",
      "\n",
      "\n",
      "{'woman', 'is', 'violin', 'playing', 'the'}\n",
      "{'to', 'guitar', 'lady', 'enjoys', 'young', 'listening', 'the'}\n",
      "\n",
      "\n",
      "{'of', 'dawn', 'a', 'back', 'with', 'group', 'whole', 'friends', 'riding', 'at', 'went', 'John', 'horse'}\n",
      "{'to', 'it', 'dawn', 'a', 'is', 'you', 'early', 'in', 'if', 'magnificent', 'up', 'enough', 'at', 'take', 'wake', 'for', 'view', 'Sunrise'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# full path of the corpus file, assuming that the trial folder containing the input file is in the same directory as the \"ipython\" file\n",
    "absolute_file_path = os.path.dirname(os.path.abspath(\"__file__\")) + \"//trial//STS.input.txt\"\n",
    "\n",
    "# find all sentence pairs in the document\n",
    "sentence_pairs = []\n",
    "sentence_set_pairs = []\n",
    "with open(absolute_file_path) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        index, sentence0, sentence1 = line.split(\"\\t\")\n",
    "        sentence_pairs.append((get_named_entity_chunked_sentence(sentence0), get_named_entity_chunked_sentence(sentence1)))\n",
    "        print(\"First sentence: \\t\", sentence0, \"\\nSecond sentence: \\t\", sentence1, \"\\n\")\n",
    "    print()  \n",
    "    \n",
    "# the pairs of sentences are shown\n",
    "for pair in sentence_pairs:\n",
    "    print(\"{}\\n{}\\n\\n\".format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentence similarity calculation using words and named entities, compared with the gold standard.\n",
    "The pairs of sentences are checked to see how similar they are, using the Jaccard distance: the more words or named entities two sentences share, the more alike they are considered to be. The pairs are shown along with their distance and dissimilarity score (scaled to be comparable with the gold standard one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence words and named entities:  {'is', 'bathing', 'in', 'sink', 'bird', 'the'} \n",
      "Second sentence words and named entities:  {'basin', 'itself', 'is', 'washing', 'in', 'Birdie', 'water', 'the'} \n",
      "Word-and-named-entity-based distance: 0.727 \n",
      "Word-and-named-entity-based dissimilarity score: 4 \n",
      "Gold standard dissimilarity score: 5 \n",
      "\n",
      "First sentence words and named entities:  {'to', 'invade', 'may', 'in', 'attempted', '2010', 'troops', 'Kabul', 'the'} \n",
      "Second sentence words and named entities:  {'invaded', 'last', 'may', '7th', '2010', 'year', 'US', 'on', 'army', 'Kabul', 'the'} \n",
      "Word-and-named-entity-based distance: 0.75 \n",
      "Word-and-named-entity-based dissimilarity score: 4 \n",
      "Gold standard dissimilarity score: 4 \n",
      "\n",
      "First sentence words and named entities:  {'not', 'is', 'a', 'he', 'considered', 'witness', 'said', 'suspect', 'John', 'but'} \n",
      "Second sentence words and named entities:  {'not', 'is', 'a', 'he', 'anymore', 'said', 'suspect', 'John'} \n",
      "Word-and-named-entity-based distance: 0.364 \n",
      "Word-and-named-entity-based dissimilarity score: 2 \n",
      "Gold standard dissimilarity score: 3 \n",
      "\n",
      "First sentence words and named entities:  {'of', 'they', 'out', 'groups', 'in', 'flew', 'nest', 'the'} \n",
      "Second sentence words and named entities:  {'they', 'into', 'flew', 'nest', 'together', 'the'} \n",
      "Word-and-named-entity-based distance: 0.6 \n",
      "Word-and-named-entity-based dissimilarity score: 3 \n",
      "Gold standard dissimilarity score: 2 \n",
      "\n",
      "First sentence words and named entities:  {'woman', 'is', 'violin', 'playing', 'the'} \n",
      "Second sentence words and named entities:  {'to', 'guitar', 'lady', 'enjoys', 'young', 'listening', 'the'} \n",
      "Word-and-named-entity-based distance: 0.909 \n",
      "Word-and-named-entity-based dissimilarity score: 5 \n",
      "Gold standard dissimilarity score: 1 \n",
      "\n",
      "First sentence words and named entities:  {'of', 'dawn', 'a', 'back', 'with', 'group', 'whole', 'friends', 'riding', 'at', 'went', 'John', 'horse'} \n",
      "Second sentence words and named entities:  {'to', 'it', 'dawn', 'a', 'is', 'you', 'early', 'in', 'if', 'magnificent', 'up', 'enough', 'at', 'take', 'wake', 'for', 'view', 'Sunrise'} \n",
      "Word-and-named-entity-based distance: 0.893 \n",
      "Word-and-named-entity-based dissimilarity score: 4 \n",
      "Gold standard dissimilarity score: 0 \n",
      "\n",
      "Pearson correlation between word-and-named-entity-based method and gold standard: -0.207\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import jaccard_distance\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# gold standard file path\n",
    "absolute_file_path = os.path.dirname(os.path.abspath(\"__file__\")) + \"//trial//STS.gs.txt\"\n",
    "\n",
    "# get the gold standard scores\n",
    "gold_scores = []\n",
    "with open(absolute_file_path) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        _, score = line.split(\"\\t\")\n",
    "        gold_scores.append(int(score))\n",
    "        \n",
    "word_ne_scores = []\n",
    "\n",
    "# compute the Jaccard distance to see how similar or different two sentences are\n",
    "for i in range(len(sentence_pairs)):\n",
    "    pair = sentence_pairs[i]\n",
    "    word_ne_dist = jaccard_distance(set(pair[0]), set(pair[1]))\n",
    "    word_ne_score = round(word_ne_dist * 5)\n",
    "    word_ne_scores.append(word_ne_score)\n",
    "    print(\"First sentence words and named entities: \", pair[0], \"\\nSecond sentence words and named entities: \", pair[1], \"\\nWord-and-named-entity-based distance:\", round(word_ne_dist, 3), \"\\nWord-and-named-entity-based dissimilarity score:\", word_ne_score, \"\\nGold standard dissimilarity score:\", gold_scores[i], \"\\n\") \n",
    "\n",
    "# Pearson correlation between the tested and the gold standard scores\n",
    "word_ne_pearson = pearsonr(word_ne_scores, gold_scores)\n",
    "print(\"Pearson correlation between word-and-named-entity-based method and gold standard:\", round(word_ne_pearson[0], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explanation of the difference between using words along with named entities, and the gold standard results.\n",
    "The scores obtained with our current approach (which considers words and named entities) are very different from those present in the gold standard: the Pearson correlation is roughly 0.21, with 0 meaning no correlation and 1 complete correlation. This happens because the gold standard uses more advanced techniques to determine how similar two sentences are. Particularly, the word-based method uses only word tokenization and the grouping of words that represent a same concept, such as a person's name with surname.\n",
    "\n",
    "If we put the results into context with previous sessions, we recall that using only words gave the exact same result for the trial sentences (0.21 correlation), while using lemmas (0.36) and synsets (0.50) proved to more more accurate. We get the same result as with only words since the 5 pairs of trial sentences do not contain any multi-term named entity.\n",
    "\n",
    "To evaluate with more rigour the goodness of using named entities along with words, however, we should check the results applied to the training data of the IHLT course project, which contains a higher number of sentences, some of which can contain multi-term named entities. In those particular cases, there will be a match between the named entities (and thus the similarity calculus will be affected) only if both sentences in a pair have the full multi-term named entity in them. If one has \"John Smith\" but the other has \"John\" a match will not be found, for example, which would not have happened with only words, in which case the \"John\" word would have matched while not the \"Smith\" one. In some cases they might be referring to the same person even if one uses only the name and the other adds the surname, and the named-entity-based approach used in this session would fail to identify this, being too strict.\n",
    "\n",
    "It is not relevant to compute named entities if they are going to be tested just as a lexical level along words, as used here. In such case, using words will give an equal or similar result in most sentence comparison cases, and will avoid the extra computation cost of named-entity recognition.\n",
    "\n",
    "Using named entities would be more intersting as a combination with semantic approaches such as the synset-comparison techniques used in passed sessions, that proved to be alone more accurate than the purely lexical and syntactic methodologies. Named entity tagging gives the type of conceptual element a named entity is (e.g. a person or an organization), which, along with synsets, would give intersting semantic information of a sentence, enabling for a better comparison with others. The impact of using named entities, however, would be only of truly relevant significance if there is a high frequency of named entities within the tested set of sentence pairs to compare."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
