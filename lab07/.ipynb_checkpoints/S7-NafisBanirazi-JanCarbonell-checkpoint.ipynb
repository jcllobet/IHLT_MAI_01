{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Exercise - Session 7\n",
    "\n",
    "### Students: Nafis Banirazi & Jan Carbonell\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this lab is toread all the pairs of sentences, compute their similarities using words + Nammed Enntities (NEs) and Jaccard Coefficient and show the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/jan/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/jan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# initial imports. Could also be done in the PC\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "#additional set of imports\n",
    "from nltk import pos_tag\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.parse import CoreNLPParser\n",
    "\n",
    "# Core Named-entity tagger as stanford one is deprecated: https://github.com/nltk/nltk/issues/2010\n",
    "tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
    "\n",
    "# launch the server in terminal\n",
    "# java -mx4g -cp \"/home/jan/Downloads/stanford-corenlp-full-2018-10-05/*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mark Pedersen', 'and', 'John Smith', 'are', 'working', 'at', 'Google', 'since', '1994', 'for', '1000', '$', 'per', 'week']\n"
     ]
    }
   ],
   "source": [
    "def get_stanford_named_entity_chunked(sentence):\n",
    "    \"\"\"Given the passed sentence string, returns an array with the chunks (words and named entities) it contains, using Stanford NLP\"\"\"\n",
    "        \n",
    "    # obtain an array with the sentence tokens\n",
    "    tokenized_s = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # tag and run as a normal word or a named entity (e.g. a person or an organization)\n",
    "    tagged_s = tagger.tag(tokenized_s)\n",
    "    \n",
    "    chunked_sentence = []\n",
    "    last_token = ''\n",
    "    last_tag = ''\n",
    "    \n",
    "    for tagged_token in tagged_s:\n",
    "        \n",
    "        token = tagged_token[0]\n",
    "        tag = tagged_token[1]\n",
    "        \n",
    "        # make normal words have lower case, also discard punctuation marks\n",
    "        if tag == 'O':\n",
    "            if token.isalnum():\n",
    "                chunked_sentence.append(token.lower())\n",
    "         \n",
    "        # keep named entities with the original capitalization\n",
    "        else:\n",
    "            if last_tag == tag:\n",
    "                chunked_sentence[-1] += ' ' + token\n",
    "            else:\n",
    "                chunked_sentence.append(token)\n",
    "        \n",
    "        last_token = token\n",
    "        last_tag = tag\n",
    "    \n",
    "    return chunked_sentence\n",
    "\n",
    "# example: note it does not group the terms of named entities, always 1 by 1. \n",
    "print(get_stanford_named_entity_chunked(\"Mark Pedersen and John Smith are working at Google since 1994 for 1000$ per week.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mark Pedersen', 'and', 'John Smith', 'are', 'working', 'at', 'Google', 'since', '1994', 'for', '1000', 'per', 'week']\n"
     ]
    }
   ],
   "source": [
    "def get_ne_chunked(sentence):\n",
    "    \"\"\"Given the passed sentence string, returns an array with the chunks (words and named entities) it contains, using NLTK\"\"\"\n",
    "        \n",
    "    # obtain an array with the sentence tokens\n",
    "    tokenized_s = nltk.word_tokenize(sentence)\n",
    "        \n",
    "    # chunk and tag with NLTK named-entity tagged\n",
    "    x = pos_tag(word_tokenize(sentence))\n",
    "    chunk_tree = ne_chunk(x, binary=True)\n",
    "    \n",
    "    chunked_sentence = []\n",
    "    for chunk in chunk_tree:\n",
    "        \n",
    "        # keep named entities with the original capitalization\n",
    "        if hasattr(chunk, 'label'):\n",
    "            token = ' '.join(term[0] for term in chunk)\n",
    "            chunked_sentence.append(token)\n",
    "            \n",
    "        # make normal words have lower case, also discard puntuaction marks\n",
    "        else:\n",
    "            token = chunk[0]\n",
    "            if token.isalnum():\n",
    "                chunked_sentence.append(token.lower())\n",
    "    \n",
    "    return chunked_sentence\n",
    "\n",
    "# example: note that NLTK naming-entity tagger does group the terms of named entities in a single string\n",
    "print(get_ne_chunked(\"Mark Pedersen and John Smith are working at Google since 1994 for 1000$ per week.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to open and read the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence: \t The bird is bathing in the sink. \n",
      "Second sentence: \t Birdie is washing itself in the water basin.\n",
      " \n",
      "\n",
      "First sentence: \t In May 2010, the troops attempted to invade Kabul. \n",
      "Second sentence: \t The US army invaded Kabul on May 7th last year, 2010.\n",
      " \n",
      "\n",
      "First sentence: \t John said he is considered a witness but not a suspect. \n",
      "Second sentence: \t \"He is not a suspect anymore.\" John said.\n",
      " \n",
      "\n",
      "First sentence: \t They flew out of the nest in groups. \n",
      "Second sentence: \t They flew into the nest together.\n",
      " \n",
      "\n",
      "First sentence: \t The woman is playing the violin. \n",
      "Second sentence: \t The young lady enjoys listening to the guitar.\n",
      " \n",
      "\n",
      "First sentence: \t John went horse back riding at dawn with a whole group of friends. \n",
      "Second sentence: \t Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# full path of the corpus file, with that the trial folder containing the input file being in the same directory as the JPN\n",
    "absolute_file_path = os.path.dirname(os.path.abspath(\"__file__\")) + \"//trial//STS.input.txt\"\n",
    "\n",
    "#value initialization and instantiation\n",
    "d = {}\n",
    "tests = []\n",
    "standard = []\n",
    "\n",
    "# find all sentence pairs in the document\n",
    "sentence_pairs = []\n",
    "sentence_set_pairs = []\n",
    "with open(absolute_file_path) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        index, sentence0, sentence1 = line.split(\"\\t\")\n",
    "        if index in d:\n",
    "            d[index] = sentence0, sentence1\n",
    "        else:\n",
    "            d[index] = (get_stanford_named_entity_chunked(sentence0), get_stanford_named_entity_chunked(sentence1))\n",
    "            print(\"First sentence: \\t\", sentence0, \"\\nSecond sentence: \\t\", sentence1, \"\\n\")\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentence similarity calculation using words and named entities, compared with the gold standard.\n",
    "The pairs of sentences are checked to see how similar they are, using the Jaccard distance: the more words or named entities two sentences share, the more alike they are considered to be. The pairs are shown along with their distance and dissimilarity score (scaled to be comparable with the gold standard one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence words and named entities:  ['the', 'bird', 'is', 'bathing', 'in', 'the', 'sink'] \n",
      "Second sentence words and named entities:  ['birdie', 'is', 'washing', 'itself', 'in', 'the', 'water', 'basin'] \n",
      "Word-and-named-entity-based distance: 0.727 \n",
      "Word-and-named-entity-based dissimilarity score: 4 \n",
      "Gold standard dissimilarity score: 5 \n",
      "\n",
      "First sentence words and named entities:  ['in', 'May 2010', 'the', 'troops', 'attempted', 'to', 'invade', 'Kabul'] \n",
      "Second sentence words and named entities:  ['the', 'US', 'army', 'invaded', 'Kabul', 'on', 'May 7th last year , 2010'] \n",
      "Word-and-named-entity-based distance: 0.846 \n",
      "Word-and-named-entity-based dissimilarity score: 4 \n",
      "Gold standard dissimilarity score: 4 \n",
      "\n",
      "First sentence words and named entities:  ['John', 'said', 'he', 'is', 'considered', 'a', 'witness', 'but', 'not', 'a', 'suspect'] \n",
      "Second sentence words and named entities:  ['he', 'is', 'not', 'a', 'suspect', 'anymore', 'John', 'said'] \n",
      "Word-and-named-entity-based distance: 0.364 \n",
      "Word-and-named-entity-based dissimilarity score: 2 \n",
      "Gold standard dissimilarity score: 3 \n",
      "\n",
      "First sentence words and named entities:  ['they', 'flew', 'out', 'of', 'the', 'nest', 'in', 'groups'] \n",
      "Second sentence words and named entities:  ['they', 'flew', 'into', 'the', 'nest', 'together'] \n",
      "Word-and-named-entity-based distance: 0.6 \n",
      "Word-and-named-entity-based dissimilarity score: 3 \n",
      "Gold standard dissimilarity score: 2 \n",
      "\n",
      "First sentence words and named entities:  ['the', 'woman', 'is', 'playing', 'the', 'violin'] \n",
      "Second sentence words and named entities:  ['the', 'young', 'lady', 'enjoys', 'listening', 'to', 'the', 'guitar'] \n",
      "Word-and-named-entity-based distance: 0.909 \n",
      "Word-and-named-entity-based dissimilarity score: 5 \n",
      "Gold standard dissimilarity score: 1 \n",
      "\n",
      "First sentence words and named entities:  ['John', 'went', 'horse', 'back', 'riding', 'at', 'dawn', 'with', 'a', 'whole', 'group', 'of', 'friends'] \n",
      "Second sentence words and named entities:  ['sunrise', 'at', 'dawn', 'is', 'a', 'magnificent', 'view', 'to', 'take', 'in', 'if', 'you', 'wake', 'up', 'early', 'enough', 'for', 'it'] \n",
      "Word-and-named-entity-based distance: 0.893 \n",
      "Word-and-named-entity-based dissimilarity score: 4 \n",
      "Gold standard dissimilarity score: 0 \n",
      "\n",
      "Pearson correlation between word-and-named-entity-based method and gold standard: -0.207\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import jaccard_distance\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# gold standard file path\n",
    "absolute_file_path = os.path.dirname(os.path.abspath(\"__file__\")) + \"//trial//STS.gs.txt\"\n",
    "\n",
    "# get the gold standard scores\n",
    "gold_scores = []\n",
    "with open(absolute_file_path) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        _, score = line.split(\"\\t\")\n",
    "        gold_scores.append(int(score))\n",
    "        \n",
    "word_ne_scores = []\n",
    "\n",
    "# compute the Jaccard distance to see how similar or different two sentences are\n",
    "for i in range(len(sentence_pairs)):\n",
    "    pair = sentence_pairs[i]\n",
    "    word_ne_dist = jaccard_distance(set(pair[0]), set(pair[1]))\n",
    "    word_ne_score = round(word_ne_dist * 5)\n",
    "    word_ne_scores.append(word_ne_score)\n",
    "    print(\"First sentence words and named entities: \", pair[0], \"\\nSecond sentence words and named entities: \", pair[1], \"\\nWord-and-named-entity-based distance:\", round(word_ne_dist, 3), \"\\nWord-and-named-entity-based dissimilarity score:\", word_ne_score, \"\\nGold standard dissimilarity score:\", gold_scores[i], \"\\n\") \n",
    "\n",
    "# Pearson correlation between the tested and the gold standard scores\n",
    "word_ne_pearson = pearsonr(word_ne_scores, gold_scores)\n",
    "print(\"Pearson correlation between word-and-named-entity-based method and gold standard:\", round(word_ne_pearson[0], 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
