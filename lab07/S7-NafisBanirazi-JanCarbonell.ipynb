{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Exercise - Session 7\n",
    "\n",
    "### Students: Nafis Banirazi & Jan Carbonell\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this lab is toread all the pairs of sentences, compute their similarities using words + Nammed Enntities (NEs) and Jaccard Coefficient and show the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports. Could also be done in the PC\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "#additional set of imports\n",
    "from nltk import pos_tag\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.parse import CoreNLPParser\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Core Named-entity tagger as stanford one is deprecated: https://github.com/nltk/nltk/issues/2010\n",
    "tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
    "\n",
    "# launch the server in terminal\n",
    "# java -mx4g -cp \"/home/jan/Downloads/stanford-corenlp-full-2018-10-05/*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stanford_named_entity_chunked(sentence):\n",
    "    \"\"\"Given the passed sentence string, returns an array with the chunks (words and named entities) it contains, using Stanford NLP\"\"\"\n",
    "        \n",
    "    # obtain an array with the sentence tokens\n",
    "    tokenized_s = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # tag and run as a normal word or a named entity (e.g. a person or an organization)\n",
    "    tagged_s = tagger.tag(tokenized_s)\n",
    "    \n",
    "    chunked_sentence = []\n",
    "    last_token = ''\n",
    "    last_tag = ''\n",
    "    \n",
    "    for tagged_token in tagged_s:\n",
    "        \n",
    "        token = tagged_token[0]\n",
    "        tag = tagged_token[1]\n",
    "        \n",
    "        # make normal words have lower case, also discard punctuation marks\n",
    "        if tag == 'O':\n",
    "            if token.isalnum():\n",
    "                chunked_sentence.append(token.lower())\n",
    "         \n",
    "        # keep named entities with the original capitalization\n",
    "        else:\n",
    "            if last_tag == tag:\n",
    "                chunked_sentence[-1] += ' ' + token\n",
    "            else:\n",
    "                chunked_sentence.append(token)\n",
    "        \n",
    "        last_token = token\n",
    "        last_tag = tag\n",
    "    \n",
    "    return chunked_sentence\n",
    "\n",
    "# example: note it does not group the terms of named entities, always 1 by 1. \n",
    "print(get_stanford_named_entity_chunked(\"Mark Pedersen and John Smith are working at Google since 1994 for 1000$ per week.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ne_chunked(sentence):\n",
    "    \"\"\"Given the passed sentence string, returns an array with the chunks (words and named entities) it contains, using NLTK\"\"\"\n",
    "        \n",
    "    # obtain an array with the sentence tokens\n",
    "    tokenized_s = nltk.word_tokenize(sentence)\n",
    "        \n",
    "    # chunk and tag with NLTK named-entity tagged\n",
    "    x = pos_tag(word_tokenize(sentence))\n",
    "    chunk_tree = ne_chunk(x, binary=True)\n",
    "    \n",
    "    chunked_sentence = []\n",
    "    for chunk in chunk_tree:\n",
    "        \n",
    "        # keep named entities with the original capitalization\n",
    "        if hasattr(chunk, 'label'):\n",
    "            token = ' '.join(term[0] for term in chunk)\n",
    "            chunked_sentence.append(token)\n",
    "            \n",
    "        # make normal words have lower case, also discard puntuaction marks\n",
    "        else:\n",
    "            token = chunk[0]\n",
    "            if token.isalnum():\n",
    "                chunked_sentence.append(token.lower())\n",
    "    \n",
    "    return chunked_sentence\n",
    "\n",
    "# example: note that NLTK naming-entity tagger does group the terms of named entities in a single string\n",
    "print(get_ne_chunked(\"Mark Pedersen and John Smith are working at Google since 1994 for 1000$ per week.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to open and read the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# full path of the corpus file, with that the trial folder containing the input file being in the same directory as the JPN\n",
    "absolute_file_path = os.path.dirname(os.path.abspath(\"__file__\")) + \"/./trial//STS.input.txt\"\n",
    "\n",
    "#value initialization and instantiation\n",
    "d = {}\n",
    "tests = []\n",
    "standard = []\n",
    "\n",
    "# find all sentence pairs in the document\n",
    "sentence_pairs = []\n",
    "sentence_set_pairs = []\n",
    "with open(absolute_file_path) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        index, sentence0, sentence1 = line.split(\"\\t\")\n",
    "        if index in d:\n",
    "            d[index] = sentence0, sentence1\n",
    "        else:\n",
    "            d[index] = (get_stanford_named_entity_chunked(sentence0), get_stanford_named_entity_chunked(sentence1))\n",
    "            print(\"First sentence: \\t\", sentence0, \"\\nSecond sentence: \\t\", sentence1, \"\\n\")\n",
    "    print()  \n",
    "    \n",
    "# double checking the format\n",
    "for key in d:\n",
    "    print(\"{}\\n{}\\n\\n\".format(d[key][0], d[key][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentence similarity calculation and comparison with the gold standard.\n",
    "The pairs of sentences are checked to see how similar they are, using the Jaccard distance. Previously the sentences must have been tokenized and we have picked the **Sets**, *unique values of those tokenized sentences*; The more words or named entities two sentences have in common, the more similar they are. Then we calculate the similarity as 1-JD. \n",
    "\n",
    "We compute the Jaccard distance. In this step, we must first tokenize the sentences. We then take the **sets**; *unique values of those tokenized sentences*, lemmatize them and compute the **jaccard similarity as 1 - jaccard distance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in d:\n",
    "        \n",
    "    w1 = set(d[key][0])\n",
    "    w2 = set(d[key][1])\n",
    "\n",
    "    # jaccard similarity 1 - jaccard distance\n",
    "    dist = jaccard_distance(w1, w2)\n",
    "    jaccard_similarity = 1 - dist\n",
    "    tests.append(round(jaccard_similarity,3))\n",
    "print(tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare the results with those in session 2 (document) and 3 (morphology) in which words and lemmas were considered.\n",
    "The gold standard results would be 5,4,3,2,1,0.\n",
    "\n",
    "\n",
    "In session 2 we got the following results:\n",
    "\n",
    "* *'id1': 0.308* --> 1.54\n",
    "* *'id2': 0.263* --> 1.31\n",
    "* *'id3': 0.467* --> 2.34\n",
    "* *'id4': 0.455* --> 2.27\n",
    "* *'id5': 0.231* --> 1.15\n",
    "* *'id6': 0.138* --> 0.69\n",
    "\n",
    "In session 3, using lemmas similarity, we obtained:\n",
    "* *'id1': 0.333* --> 1.67\n",
    "* *'id2': 0.412* --> 2.06\n",
    "* *'id3': 0.571* --> 2.86\n",
    "* *'id4': 0.333* --> 1.67\n",
    "* *'id5': 0.167* --> 0.84\n",
    "* *'id6': 0.138* --> 0.69\n",
    "\n",
    "In session 6, using lesk, we obtained:\n",
    "* *'id1': 0.333* --> 1.67\n",
    "* *'id2': 0.375* --> 1.88\n",
    "* *'id3': 0.571* --> 2.86\n",
    "* *'id4': 0.273* --> 1.36\n",
    "* *'id5': 0.167* --> 0.84\n",
    "* *'id6': 0.103* --> 0.52\n",
    "\n",
    "And our current result is:\n",
    "* *'id1': 0.273* --> 1.37\n",
    "* *'id2': 0.154* --> 0.77\n",
    "* *'id3': 0.636* --> 3.18\n",
    "* *'id4': 0.400* --> 2.00\n",
    "* *'id5': 0.091* --> 0.09\n",
    "* *'id6': 0.107* --> 0.53\n",
    "\n",
    "Overall, the first and second result decreased -which is a pitty- since we managed to significantly imporve our results on id3 and id4. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare the results with gold standard by giving the pearson correlation between them.\n",
    "And now, we open the Golden Standard file and calculate the perason correlation with and without lemmatization. \n",
    "\n",
    "**Pearson Correlation**\n",
    "It shows the linear relationship between two sets of data. That means: the strength of the association between the two variables. It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation\n",
    "\n",
    "**Coefficient Value** -- Strength of Association\n",
    "0.1 < | r | < .3 -- small correlation\n",
    "0.3 < | r | < .5 -- medium/moderate correlation\n",
    "| r | > .5 -- large/strong correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open('./trial/STS.gs.txt','r'):\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    standard.append(int(line[1]))\n",
    "\n",
    "a = pearsonr(standard[0:6], tests[0:6])[0]\n",
    "print('Pearson correlation:', round(a,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "\n",
    "Over the development of this lab, we have computed the similarities between different pairs of sentences; calculated their jaccard distance.\n",
    "\n",
    "We have also compared them with the gold standard and have achieved lower results than in the previous labs:\n",
    "- Pearson correlation without lemmatization: 0.396\n",
    "- Pearson correlation with lemmatization: 0.578\n",
    "- Pearson correlation with lesk: 0.668\n",
    "- **Pearson correlation using nammed entities: 0.318**\n",
    "\n",
    "**However, it is important to note that this results would be more accurate that the one we obtained in S2, S3 and S6 since we removed the punctuation**. Performance has decreased but are now focusing in what actually matters. \n",
    "\n",
    "What we can already see is that in order to tweak the performance of each algorithm, we will have to sort out their order and application so that we obtain the best possible results for the project. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
